[
    {
        "id": "2312.16927",
        "submitter": "Nikita Mokrenko",
        "authors": "Marina Kholod, Nikita Mokrenko",
        "title": "Development of Choice Model for Brand Evaluation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.EM",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Consumer choice modeling takes center stage as we delve into understanding\nhow personal preferences of decision makers (customers) for products influence\ndemand at the level of the individual. The contemporary choice theory is built\nupon the characteristics of the decision maker, alternatives available for the\nchoice of the decision maker, the attributes of the available alternatives and\ndecision rules that the decision maker uses to make a choice. The choice set in\nour research is represented by six major brands (products) of laundry\ndetergents in the Japanese market. We use the panel data of the purchases of 98\nhouseholds to which we apply the hierarchical probit model, facilitated by a\nMarkov Chain Monte Carlo simulation (MCMC) in order to evaluate the brand\nvalues of six brands. The applied model also allows us to evaluate the tangible\nand intangible brand values. These evaluated metrics help us to assess the\nbrands based on their tangible and intangible characteristics. Moreover,\nconsumer choice modeling also provides a framework for assessing the\nenvironmental performance of laundry detergent brands as the model uses the\ninformation on components (physical attributes) of laundry detergents.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 28 Dec 2023 09:51:46 GMT"
            }
        ],
        "update_date": "2023-12-29",
        "authors_parsed": [
            [
                "Kholod",
                "Marina",
                ""
            ],
            [
                "Mokrenko",
                "Nikita",
                ""
            ]
        ]
    },
    {
        "id": "2401.13370",
        "submitter": "Niccol\\`o Salvini Dr.",
        "authors": "G. Arbia, V. Nardelli, N. Salvini and I. Valentini",
        "title": "New accessibility measures based on unconventional big data sources",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.EM",
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  In health econometric studies we are often interested in quantifying aspects\nrelated to the accessibility to medical infrastructures. The increasing\navailability of data automatically collected through unconventional sources\n(such as webscraping, crowdsourcing or internet of things) recently opened\npreviously unconceivable opportunities to researchers interested in measuring\naccessibility and to use it as a tool for real-time monitoring, surveillance\nand health policies definition. This paper contributes to this strand of\nliterature proposing new accessibility measures that can be continuously feeded\nby automatic data collection. We present new measures of accessibility and we\nillustrate their use to study the territorial impact of supply-side shocks of\nhealth facilities. We also illustrate the potential of our proposal with a case\nstudy based on a huge set of data (related to the Emergency Departments in\nMilan, Italy) that have been webscraped for the purpose of this paper every 5\nminutes since November 2021 to March 2022, amounting to approximately 5 million\nobservations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 24 Jan 2024 10:59:58 GMT"
            }
        ],
        "update_date": "2024-01-25",
        "authors_parsed": [
            [
                "Arbia",
                "G.",
                ""
            ],
            [
                "Nardelli",
                "V.",
                ""
            ],
            [
                "Salvini",
                "N.",
                ""
            ],
            [
                "Valentini",
                "I.",
                ""
            ]
        ]
    },
    {
        "id": "1903.11383",
        "submitter": "Sergei Kulakov",
        "authors": "Sergei Kulakov and Florian Ziel",
        "title": "Determining Fundamental Supply and Demand Curves in a Wholesale\n  Electricity Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.GN q-fin.EC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we develop a novel method of wholesale electricity market\nmodeling. Our optimization-based model decomposes wholesale supply and demand\ncurves into buy and sell orders of individual market participants. In doing so,\nthe model detects and removes arbitrage orders. As a result, we construct an\ninnovative fundamental model of a wholesale electricity market. First, our\nfundamental demand curve has a unique composition. The demand curve lies in\nbetween the wholesale demand curve and a perfectly inelastic demand curve.\nSecond, our fundamental supply and demand curves contain only actual (i.e.\nnon-arbitrage) transactions with physical assets on buy and sell sides. Third,\nthese transactions are designated to one of the three groups of wholesale\nelectricity market participants: retailers, suppliers, or utility companies. To\nevaluate the performance of our model, we use the German wholesale market data.\nOur fundamental model yields a more precise approximation of the actual load\nvalues than a model with perfectly inelastic demand. Moreover, we conduct a\nstudy of wholesale demand elasticities. The obtained conclusions regarding\nwholesale demand elasticity are consistent with the existing academic\nliterature.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 22 Mar 2019 11:17:52 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 15 Nov 2019 09:47:45 GMT"
            }
        ],
        "update_date": "2019-11-18",
        "authors_parsed": [
            [
                "Kulakov",
                "Sergei",
                ""
            ],
            [
                "Ziel",
                "Florian",
                ""
            ]
        ]
    },
    {
        "id": "1607.05660",
        "submitter": "Olgu Benli",
        "authors": "T. O. Benli",
        "title": "A Comparison of Nineteen Various Electricity Consumption Forecasting\n  Approaches and Practicing to Five Different Households in Turkey",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.ST",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The accuracy of the household electricity consumption forecast is vital in\ntaking better cost effective and energy efficient decisions. In order to design\naccurate, proper and efficient forecasting model, characteristics of the series\nhave to been analyzed. The source of time series data comes from Online\nEnerjisa System, the system of electrical energy provider in capital of Turkey,\nwhich consumers can reach their latest two year period electricity\nconsumptions; in our study the period was May 2014 to May 2016. Various\ntechniques had been applied in order to analyze the data; classical\ndecomposition models; standard typed and also with the centering moving average\nmethod, regression equations, exponential smoothing models and ARIMA models. In\nour study, nine teen different approaches; all of these have at least\ndiversified aspects of methodology, had been compared and the best model for\nforecasting were decided by considering the smallest values of MAPE, MAD and\nMSD. As a first step we took the time period May 2014 to May 2016 and found\npredicted value for June 2016 with the best forecasting model. After finding\nthe best forecasting model and fitted value for June 2016, than validating\nprocess had been taken place; we made comparisons to see how well the real\nvalue of June 2016 and forecasted value for that specific period matched.\nAfterwards we made electrical consumption forecast for the following 3 months;\nJune-September 2016 for each of five households individually.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 18 Jul 2016 19:10:53 GMT"
            }
        ],
        "update_date": "2016-07-20",
        "authors_parsed": [
            [
                "Benli",
                "T. O.",
                ""
            ]
        ]
    },
    {
        "id": "2201.07181",
        "submitter": "Stefano Ugolini",
        "authors": "Charles Goodhart, Donato Masciandaro, Stefano Ugolini (LEREPS)",
        "title": "Pandemic Recession and Helicopter Money: Venice, 1629--1631",
        "comments": null,
        "journal-ref": "Financial History Review, Cambridge University Press (CUP), In\n  press, pp.1-19",
        "doi": "10.1017/S0968565021000214",
        "report-no": null,
        "categories": "econ.GN q-fin.EC q-fin.GN",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyse the money-financed fiscal stimulus implemented in Venice during\nthe famine and plague of 1629--31, which was equivalent to a 'net-worth\nhelicopter money' strategy -- a monetary expansion generating losses to the\nissuer. We argue that the strategy aimed at reconciling the need to subsidize\ninhabitants suffering from containment policies with the desire to prevent an\nincrease in long-term government debt, but it generated much monetary\ninstability and had to be quickly reversed. This episode highlights the\nredistributive implications of the design of macroeconomic policies and the\nrole of political economy factors in determining such designs.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 12 Jan 2022 07:50:51 GMT"
            }
        ],
        "update_date": "2022-01-19",
        "authors_parsed": [
            [
                "Goodhart",
                "Charles",
                "",
                "LEREPS"
            ],
            [
                "Masciandaro",
                "Donato",
                "",
                "LEREPS"
            ],
            [
                "Ugolini",
                "Stefano",
                "",
                "LEREPS"
            ]
        ]
    },
    {
        "id": "1807.05265",
        "submitter": "Chung-Han Hsieh",
        "authors": "Chung-Han Hsieh, John A. Gubner, B. Ross Barmish",
        "title": "Rebalancing Frequency Considerations for Kelly-Optimal Stock Portfolios\n  in a Control-Theoretic Framework",
        "comments": "To appear in the Proceedings of the IEEE Conference on Decision and\n  Control, Miami Beach, FL, 2018",
        "journal-ref": null,
        "doi": "10.1109/CDC.2018.8619189",
        "report-no": null,
        "categories": "q-fin.PM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, motivated by the celebrated work of Kelly, we consider the\nproblem of portfolio weight selection to maximize expected logarithmic growth.\nGoing beyond existing literature, our focal point here is the rebalancing\nfrequency which we include as an additional parameter in our analysis. The\nproblem is first set in a control-theoretic framework, and then, the main\nquestion we address is as follows: In the absence of transaction costs, does\nhigh-frequency trading always lead to the best performance? Related to this is\nour prior work on betting, also in the Kelly context, which examines the impact\nof making a wager and letting it ride. Our results on betting frequency can be\ninterpreted in the context of weight selection for a two-asset portfolio\nconsisting of one risky asset and one riskless asset. With regard to the\nquestion above, our prior results indicate that it is often the case that there\nare no performance benefits associated with high-frequency trading. In the\npresent paper, we generalize the analysis to portfolios with multiple risky\nassets. We show that if there is an asset satisfying a new condition which we\ncall dominance, then an optimal portfolio consists of this asset alone; i.e.,\nthe trader has \"all eggs in one basket\" and performance becomes a constant\nfunction of rebalancing frequency. Said another way, the problem of rebalancing\nis rendered moot. The paper also includes simulations which address practical\nconsiderations associated with real stock prices and the dominant asset\ncondition.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 13 Jul 2018 19:51:52 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 21 Aug 2018 21:17:43 GMT"
            }
        ],
        "update_date": "2019-01-28",
        "authors_parsed": [
            [
                "Hsieh",
                "Chung-Han",
                ""
            ],
            [
                "Gubner",
                "John A.",
                ""
            ],
            [
                "Barmish",
                "B. Ross",
                ""
            ]
        ]
    },
    {
        "id": "2002.06555",
        "submitter": "Marco Pangallo",
        "authors": "Marco Pangallo",
        "title": "Synchronization of endogenous business cycles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.GN nlin.AO q-fin.EC q-fin.GN",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Business cycles are positively correlated (``comove'') across countries.\nHowever, standard models that attribute comovement to propagation of exogenous\nshocks struggle to generate a level of comovement that is as high as in the\ndata. In this paper, we consider models that produce business cycles\nendogenously, through some form of non-linear dynamics -- limit cycles or\nchaos. These models generate stronger comovement, because they combine shock\npropagation with synchronization of endogenous dynamics. In particular, we\nstudy a demand-driven model in which business cycles emerge from strategic\ncomplementarities within countries, synchronizing their oscillations through\ninternational trade linkages. We develop an eigendecomposition that explores\nthe interplay between non-linear dynamics, shock propagation and network\nstructure, and use this theory to understand the mechanisms of synchronization.\nNext, we calibrate the model to data on 24 countries and show that the\nempirical level of comovement can only be matched by combining endogenous\nbusiness cycles with exogenous shocks. Our results lend support to the\nhypothesis that business cycles are at least in part caused by underlying\nnon-linear dynamics.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 16 Feb 2020 11:26:09 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 4 Jan 2023 15:47:20 GMT"
            },
            {
                "version": "v3",
                "created": "Sun, 4 Jun 2023 18:39:35 GMT"
            }
        ],
        "update_date": "2023-06-06",
        "authors_parsed": [
            [
                "Pangallo",
                "Marco",
                ""
            ]
        ]
    },
    {
        "id": "2108.09690",
        "submitter": "Muhammad Anwar Hossain",
        "authors": "Muhammad Anwar Hossain and Iryna Zablotska-Manos",
        "title": "The changing dynamics of HIV/AIDS during the Covid-19 pandemic in the\n  Rohingya refugee camps in Bangladesh a call for action",
        "comments": "10 pages, no figure",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.GN q-fin.EC",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  COVID-19 pandemic has affected each and every country's health service and\nplunged refugees into the most desperate conditions. The plight of Rohingya\nrefugees is among the harshest. It has severely affected their existing HIV/STI\nprevention and management services and further increased the risk of violence\nand onward HIV transmission within the camps. In this commentary, we discuss\nthe context and the changing dynamics of HIV/AIDS during COVID-19 among the\nRohingya refugee community in Bangladesh. What we currently observe is the\nworst crisis in the Rohingya refugee camps thus far. Firstly, because of being\ndisplaced, Rohingya refugees have increased vulnerability to HIV, as well as to\nSTIs and other poor health outcomes. Secondly, for the same reason, they have\ninadequate access to HIV testing treatment and care. Not only because of their\nrefugee status but also because of the poor capacity of the host country to\nprovide services. Thirdly, a host of complex economic, socio-cultural and\nbehavioural factors exacerbate their dire situation with access to HIV testing,\ntreatment and care. And finally, the advent of the COVID-19 pandemic has\nchanged priorities in all societies, including the refugee camps. In the\ncontext of the unfolding COVID-19 crisis, more emphasis is placed on COVID-19\nrather than other health issues, which exacerbates the dire situation with HIV\ndetection, management, and prevention among Rohingya refugees. Despite the\ncommon crisis experienced by most countries around the world, the international\ncommunity has an obligation to work together to improve the life, livelihood,\nand health of those who are most vulnerable. Rohingya refugees are among them.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 22 Aug 2021 11:30:59 GMT"
            }
        ],
        "update_date": "2021-08-24",
        "authors_parsed": [
            [
                "Hossain",
                "Muhammad Anwar",
                ""
            ],
            [
                "Zablotska-Manos",
                "Iryna",
                ""
            ]
        ]
    },
    {
        "id": "2403.17605",
        "submitter": "Masaki Miyashita",
        "authors": "Masaki Miyashita, Takashi Ui",
        "title": "On the Pettis Integral Approach to Large Population Games",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.TH math.PR",
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  The analysis of large population economies with incomplete information often\nentails the integration of a continuum of random variables. We showcase the\nusefulness of the integral notion \\`a la Pettis (1938) to study such models. We\npresent several results on Pettis integrals, including convenient sufficient\nconditions for Pettis integrability and Fubini-like exchangeability formulae,\nillustrated through a running example. Building on these foundations, we\nconduct a unified analysis of Bayesian games with arbitrarily many\nheterogeneous agents. We provide a sufficient condition on payoff structures,\nunder which the equilibrium uniqueness is guaranteed across all signal\nstructures. Our condition is parsimonious, as it turns out necessary when\nstrategic interactions are undirected. We further identify the moment\nrestrictions, imposed on the equilibrium action-state joint distribution, which\nhave crucial implications for information designer's problem of persuading a\npopulation of strategically interacting agents. To attain these results, we\nintroduce and develop novel mathematical tools, built on the theory of integral\nkernels and reproducing kernel Hilbert spaces in functional analysis.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 26 Mar 2024 11:35:41 GMT"
            }
        ],
        "update_date": "2024-03-27",
        "authors_parsed": [
            [
                "Miyashita",
                "Masaki",
                ""
            ],
            [
                "Ui",
                "Takashi",
                ""
            ]
        ]
    },
    {
        "id": "2109.01725",
        "submitter": "Victor Aguirregabiria",
        "authors": "Victor Aguirregabiria, Allan Collard-Wexler, Stephen P. Ryan",
        "title": "Dynamic Games in Empirical Industrial Organization",
        "comments": "148 pages, Chapter in Handbook of Industrial Organization - Vol 4",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.EM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This survey is organized around three main topics: models, econometrics, and\nempirical applications. Section 2 presents the theoretical framework,\nintroduces the concept of Markov Perfect Nash Equilibrium, discusses existence\nand multiplicity, and describes the representation of this equilibrium in terms\nof conditional choice probabilities. We also discuss extensions of the basic\nframework, including models in continuous time, the concepts of oblivious\nequilibrium and experience-based equilibrium, and dynamic games where firms\nhave non-equilibrium beliefs. In section 3, we first provide an overview of the\ntypes of data used in this literature, before turning to a discussion of\nidentification issues and results, and estimation methods. We review different\nmethods to deal with multiple equilibria and large state spaces. We also\ndescribe recent developments for estimating games in continuous time and\nincorporating serially correlated unobservables, and discuss the use of machine\nlearning methods to solving and estimating dynamic games. Section 4 discusses\nempirical applications of dynamic games in IO. We start describing the first\nempirical applications in this literature during the early 2000s. Then, we\nreview recent applications dealing with innovation, antitrust and mergers,\ndynamic pricing, regulation, product repositioning, advertising, uncertainty\nand investment, airline network competition, dynamic matching, and natural\nresources. We conclude with our view of the progress made in this literature\nand the remaining challenges.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 3 Sep 2021 20:45:43 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 16 Sep 2021 16:26:08 GMT"
            }
        ],
        "update_date": "2021-09-17",
        "authors_parsed": [
            [
                "Aguirregabiria",
                "Victor",
                ""
            ],
            [
                "Collard-Wexler",
                "Allan",
                ""
            ],
            [
                "Ryan",
                "Stephen P.",
                ""
            ]
        ]
    },
    {
        "id": "2010.09068",
        "submitter": "Leysan Davletshina",
        "authors": "Natalia A. Sadovnikova, Leysan A. Davletshina, Olga A. Zolotareva,\n  Olga O. Lebedinskaya",
        "title": "Differentiation of subjects of the Russian Federation according to the\n  main parameters of socio-economic development",
        "comments": "This research was performed in the framework of the state task in the\n  field of scientific activity of the Ministry of Science and Higher Education\n  of the Russian Federation, project \"Development of the methodology and a\n  software platform for the construction of digital twins, intellectual\n  analysis and forecast of complex economic systems\", grant no. FSSW-2020-0008",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.GN q-fin.EC",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This article presents the results of a cluster analysis of the regions of the\nRussian Federation in terms of the main parameters of socio-economic\ndevelopment according to the data presented in the official data sources of the\nFederal State Statistics Service (Rosstat). Studied and analyzed the domestic\nand foreign (Eurostat) methodology for assessing the socio-economic development\nof territories. The aim of the study is to determine the main parameters of\nterritorial differentiation and to identify key indicators that affect the\nsocio-economic development of Russian regions. The authors have carried out a\nclassification of the constituent entities of the Russian Federation not in\nterms of territorial location and geographical features, but in terms of the\nspecifics and key parameters of the socio-economic situation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 18 Oct 2020 19:00:23 GMT"
            }
        ],
        "update_date": "2020-10-20",
        "authors_parsed": [
            [
                "Sadovnikova",
                "Natalia A.",
                ""
            ],
            [
                "Davletshina",
                "Leysan A.",
                ""
            ],
            [
                "Zolotareva",
                "Olga A.",
                ""
            ],
            [
                "Lebedinskaya",
                "Olga O.",
                ""
            ]
        ]
    },
    {
        "id": "1811.10109",
        "submitter": "Jiahua Xu",
        "authors": "Jiahua Xu, Benjamin Livshits",
        "title": "The Anatomy of a Cryptocurrency Pump-and-Dump Scheme",
        "comments": null,
        "journal-ref": "Proceedings of the 28th USENIX Security Symposium (2019) 1609-1625",
        "doi": "10.5555/3361338.3361450",
        "report-no": null,
        "categories": "q-fin.TR cs.LG",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While pump-and-dump schemes have attracted the attention of cryptocurrency\nobservers and regulators alike, this paper represents the first detailed\nempirical query of pump-and-dump activities in cryptocurrency markets. We\npresent a case study of a recent pump-and-dump event, investigate 412\npump-and-dump activities organized in Telegram channels from June 17, 2018 to\nFebruary 26, 2019, and discover patterns in crypto-markets associated with\npump-and-dump schemes. We then build a model that predicts the pump likelihood\nof all coins listed in a crypto-exchange prior to a pump. The model exhibits\nhigh precision as well as robustness, and can be used to create a simple, yet\nvery effective trading strategy, which we empirically demonstrate can generate\na return as high as 60% on small retail investments within a span of two and\nhalf months. The study provides a proof of concept for strategic crypto-trading\nand sheds light on the application of machine learning for crime detection.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 25 Nov 2018 22:09:26 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 17 Aug 2019 16:13:55 GMT"
            }
        ],
        "update_date": "2023-01-18",
        "authors_parsed": [
            [
                "Xu",
                "Jiahua",
                ""
            ],
            [
                "Livshits",
                "Benjamin",
                ""
            ]
        ]
    },
    {
        "id": "1810.10726",
        "submitter": "Vic Norton",
        "authors": "Vic Norton",
        "title": "How Not To Do Mean-Variance Analysis",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.PM q-fin.CP",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use the 2014 market history of two high-returning biotechnology\nexchange-traded funds to illustrate how ex post mean-variance analysis should\nnot be done. Unfortunately, the way it should not be done is the way it\ngenerally is done -- to our knowledge.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2018 05:35:45 GMT"
            }
        ],
        "update_date": "2018-10-26",
        "authors_parsed": [
            [
                "Norton",
                "Vic",
                ""
            ]
        ]
    },
    {
        "id": "1412.3623",
        "submitter": "Qian Feng",
        "authors": "Q. Feng, C.W. Oosterlee",
        "title": "Monte Carlo Calculation of Exposure Profiles and Greeks for Bermudan and\n  Barrier Options under the Heston Hull-White Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.CP math.NA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Valuation of Credit Valuation Adjustment (CVA) has become an important field\nas its calculation is required in Basel III, issued in 2010, in the wake of the\ncredit crisis. Exposure, which is defined as the potential future loss of a\ndefault event without any recovery, is one of the key elementsfor pricing CVA.\nThis paper provides a backward dynamics framework for assessing exposure\nprofiles of European, Bermudan and barrier options under the Heston and Heston\nHull-White asset dynamics. We discuss the potential of an efficient and\nadaptive Monte Carlo approach, the Stochastic Grid Bundling Method}(SGBM),\nwhich employs the techniques of simulation, regression and bundling. Greeks of\nthe exposure profiles can be calculated in the same backward iteration with\nlittle extra effort. Assuming independence between default event and exposure\nprofiles, we give examples of calculating exposure, CVA and Greeks for Bermudan\nand barrier options.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 11 Dec 2014 12:14:22 GMT"
            }
        ],
        "update_date": "2014-12-12",
        "authors_parsed": [
            [
                "Feng",
                "Q.",
                ""
            ],
            [
                "Oosterlee",
                "C. W.",
                ""
            ]
        ]
    },
    {
        "id": "1804.01475",
        "submitter": "Andrea Consiglio",
        "authors": "Andrea Consiglio, Michele Tumminello, Stavros A. Zenios",
        "title": "Pricing sovereign contingent convertible debt",
        "comments": "32 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": "Working Paper 16-05, The Wharton Financial Institutions Center",
        "categories": "q-fin.PR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a pricing model for Sovereign Contingent Convertible bonds\n(S-CoCo) with payment standstills triggered by a sovereign's Credit Default\nSwap (CDS) spread. We model CDS spread regime switching, which is prevalent\nduring crises, as a hidden Markov process, coupled with a mean-reverting\nstochastic process of spread levels under fixed regimes, in order to obtain\nS-CoCo prices through simulation. The paper uses the pricing model in a\nLongstaff-Schwartz American option pricing framework to compute future state\ncontingent S-CoCo prices for risk management. Dual trigger pricing is also\ndiscussed using the idiosyncratic CDS spread for the sovereign debt together\nwith a broad market index. Numerical results are reported using S-CoCo designs\nfor Greece, Italy and Germany with both the pricing and contingent pricing\nmodels.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 4 Apr 2018 15:42:35 GMT"
            }
        ],
        "update_date": "2018-04-05",
        "authors_parsed": [
            [
                "Consiglio",
                "Andrea",
                ""
            ],
            [
                "Tumminello",
                "Michele",
                ""
            ],
            [
                "Zenios",
                "Stavros A.",
                ""
            ]
        ]
    },
    {
        "id": "2202.10588",
        "submitter": "Matteo Malavasi",
        "authors": "Gareth W. Peters (1), Matteo Malavasi (2), Georgy Sofronov (3), Pavel\n  V. Shevchenko (2), Stefan Tr\\\"uck (2) and Jiwook Jang (2) ((1) Statistics &\n  Applied Probability, University of California Santa Barbara, (2) Actuarial\n  Studies and Business Analytics, Macquarie University, Australia, (3)\n  Mathematical and Physical Sciences, Macquarie University, Australia)",
        "title": "Cyber Loss Model Risk Translates to Premium Mispricing and Risk\n  Sensitivity",
        "comments": "30 pages, 34 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.RM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We focus on model risk and risk sensitivity when addressing the insurability\nof cyber risk. The standard statistical approaches to assessment of\ninsurability and potential mispricing are enhanced in several aspects involving\nconsideration of model risk. Model risk can arise from model uncertainty, and\nparameters uncertainty. We demonstrate how to quantify the effect of model risk\nin this analysis by incorporating various robust estimators for key model\nparameter estimates that apply in both marginal and joint cyber risk loss\nprocess modelling. We contrast these robust techniques with standard methods\npreviously used in studying insurabilty of cyber risk. This allows us to\naccurately assess the critical impact that robust estimation can have on tail\nindex estimation for heavy tailed loss models, as well as the effect of robust\ndependence analysis when quantifying joint loss models and insurance portfolio\ndiversification. We argue that the choice of such methods is akin to a form of\nmodel risk and we study the risk sensitivity that arise from choices relating\nto the class of robust estimation adopted and the impact of the settings\nassociated with such methods on key actuarial tasks such as premium calculation\nin cyber insurance. Through this analysis we are able to address the question\nthat, to the best of our knowledge, no other study has investigated in the\ncontext of cyber risk: is model risk present in cyber risk data, and how does\nis it translate into premium mispricing? We believe our findings should\ncomplement existing studies seeking to explore insurability of cyber losses. In\norder to ensure our findings are based on realistic industry informed loss\ndata, we have utilised one of the leading industry cyber loss datasets obtained\nfrom Advisen, which represents a comprehensive data set on cyber monetary\nlosses, from which we form our analysis and conclusions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 22 Feb 2022 00:02:07 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 15 Mar 2022 01:07:36 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 29 Mar 2023 00:28:16 GMT"
            }
        ],
        "update_date": "2023-03-30",
        "authors_parsed": [
            [
                "Peters",
                "Gareth W.",
                ""
            ],
            [
                "Malavasi",
                "Matteo",
                ""
            ],
            [
                "Sofronov",
                "Georgy",
                ""
            ],
            [
                "Shevchenko",
                "Pavel V.",
                ""
            ],
            [
                "Tr\u00fcck",
                "Stefan",
                ""
            ],
            [
                "Jang",
                "Jiwook",
                ""
            ]
        ]
    },
    {
        "id": "1712.01137",
        "submitter": "Dieter Hendricks",
        "authors": "Dieter Hendricks, Adam Cobb, Richard Everett, Jonathan Downing and\n  Stephen J. Roberts",
        "title": "Inferring agent objectives at different scales of a complex adaptive\n  system",
        "comments": "6 pages, 3 figures, NIPS 2017 Workshop on Learning in the Presence of\n  Strategic Behaviour (MLStrat)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.TR stat.ML",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a framework to study the effective objectives at different time\nscales of financial market microstructure. The financial market can be regarded\nas a complex adaptive system, where purposeful agents collectively and\nsimultaneously create and perceive their environment as they interact with it.\nIt has been suggested that multiple agent classes operate in this system, with\na non-trivial hierarchy of top-down and bottom-up causation classes with\ndifferent effective models governing each level. We conjecture that agent\nclasses may in fact operate at different time scales and thus act differently\nin response to the same perceived market state. Given scale-specific temporal\nstate trajectories and action sequences estimated from aggregate market\nbehaviour, we use Inverse Reinforcement Learning to compute the effective\nreward function for the aggregate agent class at each scale, allowing us to\nassess the relative attractiveness of feature vectors across different scales.\nDifferences in reward functions for feature vectors may indicate different\nobjectives of market participants, which could assist in finding the scale\nboundary for agent classes. This has implications for learning algorithms\noperating in this domain.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 4 Dec 2017 15:06:15 GMT"
            }
        ],
        "update_date": "2017-12-05",
        "authors_parsed": [
            [
                "Hendricks",
                "Dieter",
                ""
            ],
            [
                "Cobb",
                "Adam",
                ""
            ],
            [
                "Everett",
                "Richard",
                ""
            ],
            [
                "Downing",
                "Jonathan",
                ""
            ],
            [
                "Roberts",
                "Stephen J.",
                ""
            ]
        ]
    },
    {
        "id": "2402.15890",
        "submitter": "Sumit Goel",
        "authors": "Sumit Goel, Wade Hann-Caruthers",
        "title": "Optimality of weighted contracts for multi-agent contract design with a\n  budget",
        "comments": "26 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.TH cs.GT",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study a contract design problem between a principal and multiple agents.\nEach agent participates in an independent task with binary outcomes (success or\nfailure), in which it may exert costly effort towards improving its probability\nof success, and the principal has a fixed budget which it can use to provide\noutcome-dependent rewards to the agents. Crucially, we assume the principal\ncares only about maximizing the agents' probabilities of success, not how much\nof the budget it expends. We first show that a contract is optimal for some\nobjective if and only if it is a successful-get-everything contract. An\nimmediate consequence of this result is that piece-rate contracts and\nbonus-pool contracts are never optimal in this setting. We then show that for\nany objective, there is an optimal priority-based weighted contract, which\nassigns positive weights and priority levels to the agents, and splits the\nbudget among the highest-priority successful agents, with each such agent\nreceiving a fraction of the budget proportional to her weight. This result\nprovides a significant reduction in the dimensionality of the principal's\noptimal contract design problem and gives an interpretable and easily\nimplementable optimal contract. Finally, we discuss an application of our\nresults to the design of optimal contracts with two agents and quadratic costs.\nIn this context, we find that the optimal contract assigns a higher weight to\nthe agent whose success it values more, irrespective of the heterogeneity in\nthe agents' cost parameters. This suggests that the structure of the optimal\ncontract depends primarily on the bias in the principal's objective and is, to\nsome extent, robust to the heterogeneity in the agents' cost functions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 24 Feb 2024 19:55:29 GMT"
            }
        ],
        "update_date": "2024-02-27",
        "authors_parsed": [
            [
                "Goel",
                "Sumit",
                ""
            ],
            [
                "Hann-Caruthers",
                "Wade",
                ""
            ]
        ]
    },
    {
        "id": "2210.01846",
        "submitter": "Moritz Nikolaus Laber",
        "authors": "Moritz Laber, Peter Klimek, Martin Bruckner, Liuhuaying Yang, and\n  Stefan Thurner",
        "title": "Shock propagation from the Russia-Ukraine conflict on international\n  multilayer food production network determines global food availability",
        "comments": "36 pages, 8 figures. Nat Food (2023)",
        "journal-ref": null,
        "doi": "10.1038/s43016-023-00771-4",
        "report-no": null,
        "categories": "econ.GN physics.soc-ph q-fin.EC",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Dependencies in the global food production network can lead to shortages in\nnumerous regions, as demonstrated by the impacts of the Russia-Ukraine conflict\non global food supplies. Here, we reveal the losses of $125$ food products\nafter a localized shock to agricultural production in $192$ countries and\nterritories using a multilayer network model of trade (direct) and conversion\nof food products (indirect), thereby quantifying $10^8$ shock transmissions. We\nfind that a complete agricultural production loss in Ukraine has heterogeneous\nimpacts on other countries, causing relative losses of up to $89\\%$ in\nsunflower oil and $85\\%$ in maize via direct effects, and up to $25\\%$ in\npoultry meat via indirect impacts. Whilst previous studies often treated\nproducts in isolation and did not account for product conversion during\nproduction, our model studies the global propagation of local supply shocks\nalong both production and trade relations, allowing comparison of different\nresponse strategies.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 4 Oct 2022 18:28:58 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 8 May 2023 14:01:10 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 16 Jun 2023 13:01:12 GMT"
            }
        ],
        "update_date": "2023-06-19",
        "authors_parsed": [
            [
                "Laber",
                "Moritz",
                ""
            ],
            [
                "Klimek",
                "Peter",
                ""
            ],
            [
                "Bruckner",
                "Martin",
                ""
            ],
            [
                "Yang",
                "Liuhuaying",
                ""
            ],
            [
                "Thurner",
                "Stefan",
                ""
            ]
        ]
    },
    {
        "id": "1802.06885",
        "submitter": "Sergey Lobanov",
        "authors": "Elena Burmistrova, Sergey Lobanov",
        "title": "The Allen--Uzawa elasticity of substitution for nonhomogeneous\n  production functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.EM math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This note proves that the representation of the Allen elasticity of\nsubstitution obtained by Uzawa for linear homogeneous functions holds true for\nnonhomogeneous functions. It is shown that the criticism of the Allen-Uzawa\nelasticity of substitution in the works of Blackorby, Primont, Russell is based\non an incorrect example.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 9 Feb 2018 16:08:46 GMT"
            }
        ],
        "update_date": "2018-02-21",
        "authors_parsed": [
            [
                "Burmistrova",
                "Elena",
                ""
            ],
            [
                "Lobanov",
                "Sergey",
                ""
            ]
        ]
    },
    {
        "id": "1706.07758",
        "submitter": "Victor Olkhov",
        "authors": "Victor Olkhov",
        "title": "Non-Local Macroeconomic Transactions and Credits-Loans Surface-Like\n  Waves",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.EC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper describes surface-like waves of macroeconomic Credits-Loans\ntransactions on economic space. We use agent's risk ratings as their\ncoordinates and describe evolution of macro variables by transactions between\nagents. Aggregations of agent's variables with risk coordinates x on economic\nspace define macro variables as function of x. Aggregations of transactions\nbetween agents at point x and y determine functions of two variables (x,y) on\neconomic space. As example we study Credits transactions provided from agents\nat point x to agents at point y and thus amount of Loans received by agents at\npoint y from agents at point x at moment t during time term dt. We model\nevolution of macro transactions by hydrodynamic-like equations. Agents fill\nmacro domain on economic space that is bounded by minimum risk ratings of most\nsecure and maximum risk ratings of most risky agents. Economic and financial\nshocks can disturb steady borders of macro domain and cause perturbations of\ntransactions. Such disturbances can generate waves that can propagate along\nrisk borders alike to surface waves in fluids. As example, we describe simple\nmodel interactions between two transactions by hydrodynamic like equations in a\nclosed form. We introduce notions of \"macro accelerations\" and their potentials\nthat establish steady state distributions of transactions on economic space.\nFor this model in linear approximation we describe surface-like waves and show\nthat perturbations induced by surface-like waves can exponentially grow up\ninside macro domain and induce macro instabilities in a low risk area.\nDescription of possible steady state distributions of transactions and\nsurface-like waves on economic space might be important for macro modeling and\npolicy-making.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 29 May 2017 07:55:41 GMT"
            }
        ],
        "update_date": "2017-06-26",
        "authors_parsed": [
            [
                "Olkhov",
                "Victor",
                ""
            ]
        ]
    },
    {
        "id": "2301.04052",
        "submitter": "Ahmet Y. Aydemir",
        "authors": "A. Y. Aydemir",
        "title": "Optimal social security timing",
        "comments": "21 pages, 5 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.GN q-fin.EC",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The optimal age that a retiree claims social security retirement benefits is\nin general a complicated function of many factors. However, if the\nbeneficiary's finances and health are not the constraining factors, it is\npossible to formally derive mathematical models that maximize a well-defined\nmeasure of his total benefits. A model that takes into account various factors\nsuch as the increase in the benefits for delayed claims and the penalties for\nearly retirement, the advantages of investing some of the benefits in the\nfinancial markets, and the effects of cost-of-living adjustments shows that not\nwaiting until age 70 is almost always the better option. The optimal claiming\nage that maximizes the total benefits, however, depends on the expected market\nreturns and the rate of cost-of-living adjustments, with the higher market\nrates in general pushing the optimal age lower. The models presented here can\nbe easily tailored to address the particular circumstances and goals of any\nindividual.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 10 Jan 2023 16:08:23 GMT"
            }
        ],
        "update_date": "2023-01-11",
        "authors_parsed": [
            [
                "Aydemir",
                "A. Y.",
                ""
            ]
        ]
    },
    {
        "id": "2106.16047",
        "submitter": "Peter Tankov",
        "authors": "Peter Tankov and Laura Tinsi",
        "title": "Decision making with dynamic probabilistic forecasts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.TR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a sequential decision making process, such as renewable energy\ntrading or electrical production scheduling, whose outcome depends on the\nfuture realization of a random factor, such as a meteorological variable. We\nassume that the decision maker disposes of a dynamically updated probabilistic\nforecast (predictive distribution) of the random factor. We propose several\nstochastic models for the evolution of the probabilistic forecast, and show how\nthese models may be calibrated from ensemble forecasts, commonly provided by\nweather centers. We then show how these stochastic models can be used to\ndetermine optimal decision making strategies depending on the forecast updates.\nApplications to wind energy trading are given.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 30 Jun 2021 13:20:50 GMT"
            }
        ],
        "update_date": "2021-07-01",
        "authors_parsed": [
            [
                "Tankov",
                "Peter",
                ""
            ],
            [
                "Tinsi",
                "Laura",
                ""
            ]
        ]
    },
    {
        "id": "1707.00529",
        "submitter": "Jake Billings",
        "authors": "Jake Billings and Sebastian Del Barco",
        "title": "An Investigation into Laboucheres Betting System to Improve Odds of\n  Favorable Outcomes to Generate a Positive Externality Empirically",
        "comments": "30 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.GN",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Labouchere gambling system is hypothesized to increase the probability of\nwinning a predetermined arbitrary profit in a gambling system such as a coin\nflip or a roulette game in which both payouts and odds are 1:1. However, use of\nthe system increases the downside monetary risk in the event of a streak of\nmultiple losses. To begin, a player creates an arbitrary series of consecutive\nintegers with a sum equal to the desired profit from multiple rounds of\nbetting. Using the system, a player will either win an amount equal to the sum\nof the elements of the initial series or lose all of their available capital.\nThis sequence was simulated multiple times to determine the statistical\ncharacteristics of both the return and of the loss in an average round of\nbetting. By running the simulations of millions of rounds of Labouchere, it was\npossible to discern the probable outcomes of running the system using the\nLabouchere gambling sequence and plotting the results on a graph to map the\naverage return on the initial capital investment. The Labouchere system is very\npsychologically appealing to players because when applied over time it provides\nvery consistent linear returns. However, there is eventually a critical moment\nat which the available capital for betting is exceeded and a player loses all\nof their available capital. It was found that as the number of bets increased,\nthe outcome of applying the sequence approached zero.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 29 Jun 2017 18:17:38 GMT"
            }
        ],
        "update_date": "2017-07-04",
        "authors_parsed": [
            [
                "Billings",
                "Jake",
                ""
            ],
            [
                "Del Barco",
                "Sebastian",
                ""
            ]
        ]
    },
    {
        "id": "2110.00864",
        "submitter": "Charles Manski",
        "authors": "Charles F. Manski",
        "title": "Probabilistic Prediction for Binary Treatment Choice: with focus on\n  personalized medicine",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.EM stat.ML",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper extends my research applying statistical decision theory to\ntreatment choice with sample data, using maximum regret to evaluate the\nperformance of treatment rules. The specific new contribution is to study as-if\noptimization using estimates of illness probabilities in clinical choice\nbetween surveillance and aggressive treatment. Beyond its specifics, the paper\nsends a broad message. Statisticians and computer scientists have addressed\nconditional prediction for decision making in indirect ways, the former\napplying classical statistical theory and the latter measuring prediction\naccuracy in test samples. Neither approach is satisfactory. Statistical\ndecision theory provides a coherent, generally applicable methodology.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 2 Oct 2021 18:34:59 GMT"
            }
        ],
        "update_date": "2021-10-05",
        "authors_parsed": [
            [
                "Manski",
                "Charles F.",
                ""
            ]
        ]
    },
    {
        "id": "2201.09221",
        "submitter": "Pengoeng Yue",
        "authors": "Pengpeng Yue, Aslihan Gizem Korkmaz, Zhichao Yin, Haigang Zhou",
        "title": "The rise of digital finance: Financial inclusion or debt trap",
        "comments": null,
        "journal-ref": "Finance Research Letters, 2021, 102604",
        "doi": "10.1016/j.frl.2021.102604",
        "report-no": null,
        "categories": "econ.GN q-fin.EC",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study focuses on the impact of digital finance on households. While\ndigital finance has brought financial inclusion, it has also increased the risk\nof households falling into a debt trap. We provide evidence that supports this\nnotion and explain the channel through which digital finance increases the\nlikelihood of financial distress. Our results show that the widespread use of\ndigital finance increases credit market participation. The broadened access to\ncredit markets increases household consumption by changing the marginal\npropensity to consume. However, the easier access to credit markets also\nincreases the risk of households falling into a debt trap.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 23 Jan 2022 10:15:15 GMT"
            }
        ],
        "update_date": "2022-01-25",
        "authors_parsed": [
            [
                "Yue",
                "Pengpeng",
                ""
            ],
            [
                "Korkmaz",
                "Aslihan Gizem",
                ""
            ],
            [
                "Yin",
                "Zhichao",
                ""
            ],
            [
                "Zhou",
                "Haigang",
                ""
            ]
        ]
    },
    {
        "id": "1509.00372",
        "submitter": "Florian Ziel",
        "authors": "Florian Ziel, Rick Steinert",
        "title": "Electricity Price Forecasting using Sale and Purchase Curves: The\n  X-Model",
        "comments": "Online appendix is partially provided",
        "journal-ref": "Energy Economics, 59 (2016) 435-454",
        "doi": "10.1016/j.eneco.2016.08.008",
        "report-no": null,
        "categories": "q-fin.TR q-fin.ST",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Our paper aims to model and forecast the electricity price by taking a\ncompletely new perspective on the data. It will be the first approach which is\nable to combine the insights of market structure models with extensive and\nmodern econometric analysis. Instead of directly modeling the electricity price\nas it is usually done in time series or data mining approaches, we model and\nutilize its true source: the sale and purchase curves of the electricity\nexchange. We will refer to this new model as X-Model, as almost every\nderegulated electricity price is simply the result of the intersection of the\nelectricity supply and demand curve at a certain auction. Therefore we show an\napproach to deal with a tremendous amount of auction data, using a subtle data\nprocessing technique as well as dimension reduction and lasso based estimation\nmethods. We incorporate not only several known features, such as seasonal\nbehavior or the impact of other processes like renewable energy, but also\ncompletely new elaborated stylized facts of the bidding structure. Our model is\nable to capture the non-linear behavior of the electricity price, which is\nespecially useful for predicting huge price spikes. Using simulation methods we\nshow how to derive prediction intervals for probabilistic forecasting. We\ndescribe and show the proposed methods for the day-ahead EPEX spot price of\nGermany and Austria.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 1 Sep 2015 16:04:42 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 17 Aug 2016 09:14:29 GMT"
            }
        ],
        "update_date": "2016-10-18",
        "authors_parsed": [
            [
                "Ziel",
                "Florian",
                ""
            ],
            [
                "Steinert",
                "Rick",
                ""
            ]
        ]
    },
    {
        "id": "1902.05710",
        "submitter": "Thierry Roncalli",
        "authors": "Jean-Charles Richard, Thierry Roncalli",
        "title": "Constrained Risk Budgeting Portfolios: Theory, Algorithms, Applications\n  & Puzzles",
        "comments": "36 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.PM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article develops the theory of risk budgeting portfolios, when we would\nlike to impose weight constraints. It appears that the mathematical problem is\nmore complex than the traditional risk budgeting problem. The formulation of\nthe optimization program is particularly critical in order to determine the\nright risk budgeting portfolio. We also show that numerical solutions can be\nfound using methods that are used in large-scale machine learning problems.\nIndeed, we develop an algorithm that mixes the method of cyclical coordinate\ndescent (CCD), alternating direction method of multipliers (ADMM), proximal\noperators and Dykstra's algorithm. This theoretical body is then applied to\nsome investment problems. In particular, we show how to dynamically control the\nturnover of a risk parity portfolio and how to build smart beta portfolios\nbased on the ERC approach by improving the liquidity of the portfolio or\nreducing the small cap bias. Finally, we highlight the importance of the\nhomogeneity property of risk measures and discuss the related scaling puzzle.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 15 Feb 2019 07:41:14 GMT"
            }
        ],
        "update_date": "2019-02-18",
        "authors_parsed": [
            [
                "Richard",
                "Jean-Charles",
                ""
            ],
            [
                "Roncalli",
                "Thierry",
                ""
            ]
        ]
    },
    {
        "id": "1411.4606",
        "submitter": "Hyungbin Park",
        "authors": "Jihun Han, Hyungbin Park",
        "title": "The Intrinsic Bounds on the Risk Premium of Markovian Pricing Kernels",
        "comments": "arXiv admin note: text overlap with arXiv:1410.2282",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.MF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The risk premium is one of main concepts in mathematical finance. It is a\nmeasure of the trade-offs investors make between return and risk and is defined\nby the excess return relative to the risk-free interest rate that is earned\nfrom an asset per one unit of risk. The purpose of this article is to determine\nupper and lower bounds on the risk premium of an asset based on the market\nprices of options. One of the key assumptions to achieve this goal is that the\nmarket is Markovian. Under this assumption, we can transform the problem of\nfinding the bounds into a second-order differential equation. We then obtain\nupper and lower bounds on the risk premium by analyzing the differential\nequation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 17 Nov 2014 19:51:16 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 18 Nov 2014 23:48:58 GMT"
            },
            {
                "version": "v3",
                "created": "Sat, 6 Dec 2014 01:06:45 GMT"
            },
            {
                "version": "v4",
                "created": "Sat, 28 Mar 2015 09:41:03 GMT"
            },
            {
                "version": "v5",
                "created": "Mon, 28 Sep 2015 06:38:14 GMT"
            }
        ],
        "update_date": "2015-09-29",
        "authors_parsed": [
            [
                "Han",
                "Jihun",
                ""
            ],
            [
                "Park",
                "Hyungbin",
                ""
            ]
        ]
    },
    {
        "id": "1803.00096",
        "submitter": "Daniel Kinn",
        "authors": "Daniel Kinn",
        "title": "Synthetic Control Methods and Big Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.EM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many macroeconomic policy questions may be assessed in a case study\nframework, where the time series of a treated unit is compared to a\ncounterfactual constructed from a large pool of control units. I provide a\ngeneral framework for this setting, tailored to predict the counterfactual by\nminimizing a tradeoff between underfitting (bias) and overfitting (variance).\nThe framework nests recently proposed structural and reduced form machine\nlearning approaches as special cases. Furthermore, difference-in-differences\nwith matching and the original synthetic control are restrictive cases of the\nframework, in general not minimizing the bias-variance objective. Using\nsimulation studies I find that machine learning methods outperform traditional\nmethods when the number of potential controls is large or the treated unit is\nsubstantially different from the controls. Equipped with a toolbox of\napproaches, I revisit a study on the effect of economic liberalisation on\neconomic growth. I find effects for several countries where no effect was found\nin the original study. Furthermore, I inspect how a systematically important\nbank respond to increasing capital requirements by using a large pool of banks\nto estimate the counterfactual. Finally, I assess the effect of a changing\nproduct price on product sales using a novel scanner dataset.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 28 Feb 2018 21:32:09 GMT"
            }
        ],
        "update_date": "2018-03-02",
        "authors_parsed": [
            [
                "Kinn",
                "Daniel",
                ""
            ]
        ]
    },
    {
        "id": "2310.09295",
        "submitter": "Kira Henshaw",
        "authors": "Kira Henshaw, Jorge M. Ramirez, Jos\\'e M. Flores-Contr\\'o, Enrique A.\n  Thomann, Sooie-Hoe Loke, Corina Constantinescu",
        "title": "On the impact of insurance on households susceptible to random\n  proportional losses: An analysis of poverty trapping",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.RM math.PR stat.AP",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we consider a risk process with deterministic growth and\nmultiplicative jumps to model the capital of a low-income household. Reflecting\nthe high-risk nature of the low-income environment, capital losses are assumed\nto be proportional to the level of accumulated capital at the jump time. Our\naim is to derive the probability that a household falls below the poverty line,\ni.e. the trapping probability, where ``trapping\" occurs when the level of\ncapital of a household holds falls below the poverty line, to an area from\nwhich it is difficult to escape without external help. Considering the\nremaining proportion of capital to be distributed as a special case of the beta\ndistribution, closed-form expressions for the trapping probability are obtained\nvia analysis of the Laplace transform of the infinitesimal generator of the\nprocess. To study the impact of insurance on this probability, introduction of\nan insurance product offering proportional coverage is presented. The\ninfinitesimal generator of the insured process gives rise to non-local\ndifferential equations. To overcome this, we propose a recursive method for\nderiving a closed-form solution of the integro-differential equation associated\nwith the infinitesimal generator of the insured process and provide a numerical\nestimation method for obtaining the trapping probability. Constraints on the\nrate parameters of the process that prevent certain trapping are derived in\nboth the uninsured and insured cases using classical results from risk theory.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 22 Sep 2023 14:00:02 GMT"
            }
        ],
        "update_date": "2023-10-17",
        "authors_parsed": [
            [
                "Henshaw",
                "Kira",
                ""
            ],
            [
                "Ramirez",
                "Jorge M.",
                ""
            ],
            [
                "Flores-Contr\u00f3",
                "Jos\u00e9 M.",
                ""
            ],
            [
                "Thomann",
                "Enrique A.",
                ""
            ],
            [
                "Loke",
                "Sooie-Hoe",
                ""
            ],
            [
                "Constantinescu",
                "Corina",
                ""
            ]
        ]
    },
    {
        "id": "1006.5587",
        "submitter": "Hideaki Aoyama",
        "authors": "Hideaki Aoyama, Yoshi Fujiwara, Yuichi Ikeda, Hiroshi Iyetomi, and\n  Wataru Souma",
        "title": "Econophysics on Real Economy -The First Decade of the Kyoto Econophysics\n  Group-",
        "comments": "6 pages in Phys.Rev. format. Minor typographical errors are corrected\n  in v3",
        "journal-ref": null,
        "doi": null,
        "report-no": "KUNS-2275",
        "categories": "q-fin.GN physics.hist-ph",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Research activities of Kyoto Econophysics Group is reviewed. Strong emphasis\nhas been placed on real economy. While the initial stage of research was a\nfirst high-definition data analysis on personal income, it soon progressed to\nfirm dynamics, growth rate distribution and establishment of Pareto's law and\nGibrat's law. It then led to analysis and simulation of firm dynamics on\neconomic network. Currently it covers a wide rage of dynamics of firms and\nfinancial institutions on complex network, using Japanese large-scale network\ndata, some of which are not available in other countries. Activities of this\ngroup for publicising and promoting understanding of econophysics is also\nreviewed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 29 Jun 2010 12:18:55 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 30 Jun 2010 03:58:59 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 2 Jul 2010 02:53:21 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Aoyama",
                "Hideaki",
                ""
            ],
            [
                "Fujiwara",
                "Yoshi",
                ""
            ],
            [
                "Ikeda",
                "Yuichi",
                ""
            ],
            [
                "Iyetomi",
                "Hiroshi",
                ""
            ],
            [
                "Souma",
                "Wataru",
                ""
            ]
        ]
    },
    {
        "id": "2403.17777",
        "submitter": "Yao Luo",
        "authors": "JoonHwan Cho, Yao Luo, Ruli Xiao",
        "title": "Deconvolution from two order statistics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.EM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Economic data are often contaminated by measurement errors and truncated by\nranking. This paper shows that the classical measurement error model with\nindependent and additive measurement errors is identified nonparametrically\nusing only two order statistics of repeated measurements. The identification\nresult confirms a hypothesis by Athey and Haile (2002) for a symmetric\nascending auction model with unobserved heterogeneity. Extensions allow for\nheterogeneous measurement errors, broadening the applicability to additional\nempirical settings, including asymmetric auctions and wage offer models. We\nadapt an existing simulated sieve estimator and illustrate its performance in\nfinite samples.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 26 Mar 2024 15:09:55 GMT"
            }
        ],
        "update_date": "2024-03-27",
        "authors_parsed": [
            [
                "Cho",
                "JoonHwan",
                ""
            ],
            [
                "Luo",
                "Yao",
                ""
            ],
            [
                "Xiao",
                "Ruli",
                ""
            ]
        ]
    },
    {
        "id": "1506.08847",
        "submitter": "Provash  Mali",
        "authors": "Provash Mali and Amitabha Mukhopadhyay",
        "title": "Multifractal characterization of gold market: a multifractal detrended\n  fluctuation analysis",
        "comments": "20 pages, 7 figures, 1 table",
        "journal-ref": "Physica A 413,2014,361-372",
        "doi": "10.1016/j.physa.2014.06.076",
        "report-no": null,
        "categories": "q-fin.ST physics.data-an",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The multifractal detrended fluctuation analysis technique is employed to\nanalyze the time series of gold consumer price index (CPI) and the market trend\nof three world's highest gold consuming countries, namely China, India and\nTurkey for the period: 1993-July 2013. Various multifractal variables, such as\nthe generalized Hurst exponent, the multifractal exponent and the singularity\nspectrum, are calculated and the results are fitted to the generalized binomial\nmultifractal (GBM) series that consists of only two parameters. Special\nemphasis is given to identify the possible source(s) of multifractality in\nthese series. Our analysis shows that the CPI series and all three market\nseries are of multifractal nature. The origin of multifractality for the CPI\ntime series and Indian market series is found due to a long-range time\ncorrelation, whereas it is mostly due to the fat-tailed probability\ndistributions of the values for the Chinese and Turkey markets. The GBM model\nseries more or less describes all the time series analyzed here.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 17 May 2015 07:27:26 GMT"
            }
        ],
        "update_date": "2015-07-01",
        "authors_parsed": [
            [
                "Mali",
                "Provash",
                ""
            ],
            [
                "Mukhopadhyay",
                "Amitabha",
                ""
            ]
        ]
    },
    {
        "id": "1602.06685",
        "submitter": "Romain Blanchard",
        "authors": "Romain Blanchard, Laurence Carassus, Mikl\\'os R\\'asonyi",
        "title": "Non-concave optimal investment and no-arbitrage: a measure theoretical\n  approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.MF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider non-concave and non-smooth random utility functions with do- main\nof definition equal to the non-negative half-line. We use a dynamic pro-\ngramming framework together with measurable selection arguments to establish\nboth the no-arbitrage condition characterization and the existence of an\noptimal portfolio in a (generically incomplete) discrete-time financial market\nmodel with finite time horizon. In contrast to the existing literature, we\npropose to consider a probability space which is not necessarily complete.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 22 Feb 2016 08:54:23 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 8 Mar 2016 17:21:09 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 26 Aug 2016 08:14:20 GMT"
            }
        ],
        "update_date": "2016-08-29",
        "authors_parsed": [
            [
                "Blanchard",
                "Romain",
                ""
            ],
            [
                "Carassus",
                "Laurence",
                ""
            ],
            [
                "R\u00e1sonyi",
                "Mikl\u00f3s",
                ""
            ]
        ]
    },
    {
        "id": "1301.0719",
        "submitter": "David Hobson",
        "authors": "Han Feng and David Hobson",
        "title": "Gambling in contests with regret",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.PM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper discusses the gambling contest introduced in Seel & Strack\n(Gambling in contests, Discussion Paper Series of SFB/TR 15 Governance and the\nEfficiency of Economic Systems 375, Mar 2012.) and considers the impact of\nadding a penalty associated with failure to follow a winning strategy.\n  The Seel & Strack model consists of $n$-agents each of whom privately\nobserves a transient diffusion process and chooses when to stop it. The player\nwith the highest stopped value wins the contest, and each player's objective is\nto maximise their probability of winning the contest. We give a new derivation\nof the results of Seel & Strack based on a Lagrangian approach. Moreover, we\nconsider an extension of the problem in which in the case when an agent is\npenalised when their strategy is suboptimal, in the sense that they do not win\nthe contest, but there existed an alternative strategy which would have\nresulted in victory.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 4 Jan 2013 13:38:46 GMT"
            }
        ],
        "update_date": "2013-01-07",
        "authors_parsed": [
            [
                "Feng",
                "Han",
                ""
            ],
            [
                "Hobson",
                "David",
                ""
            ]
        ]
    },
    {
        "id": "1601.02149",
        "submitter": "Luis Zuluaga",
        "authors": "Robert Howley and Robert Storer and Juan Vera and Luis F. Zuluaga",
        "title": "Computing semiparametric bounds on the expected payments of insurance\n  instruments via column generation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.PR stat.ME",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It has been recently shown that numerical semiparametric bounds on the\nexpected payoff of fi- nancial or actuarial instruments can be computed using\nsemidefinite programming. However, this approach has practical limitations.\nHere we use column generation, a classical optimization technique, to address\nthese limitations. From column generation, it follows that practical univari-\nate semiparametric bounds can be found by solving a series of linear programs.\nIn addition to moment information, the column generation approach allows the\ninclusion of extra information about the random variable; for instance,\nunimodality and continuity, as well as the construction of corresponding\nworst/best-case distributions in a simple way.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 9 Jan 2016 19:22:27 GMT"
            }
        ],
        "update_date": "2016-01-12",
        "authors_parsed": [
            [
                "Howley",
                "Robert",
                ""
            ],
            [
                "Storer",
                "Robert",
                ""
            ],
            [
                "Vera",
                "Juan",
                ""
            ],
            [
                "Zuluaga",
                "Luis F.",
                ""
            ]
        ]
    },
    {
        "id": "1603.05513",
        "submitter": "Claudio Altafini",
        "authors": "Claudio Altafini",
        "title": "The geometric phase of stock trading",
        "comments": "15 pages, 12 figures",
        "journal-ref": "PLoS ONE, vol 11, e0161538, 2016",
        "doi": "10.1371/journal.pone.0161538",
        "report-no": null,
        "categories": "q-fin.TR q-fin.CP",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Geometric phases describe how in a continuous-time dynamical system the\ndisplacement of a variable (called phase variable) can be related to other\nvariables (shape variables) undergoing a cyclic motion, according to an area\nrule. The aim of this paper is to show that geometric phases can exist also for\ndiscrete-time systems, and even when the cycles in shape space have zero area.\nA context in which this principle can be applied is stock trading. A zero-area\ncycle in shape space represents the type of trading operations normally carried\nout by high-frequency traders (entering and exiting a position on a fast\ntime-scale), while the phase variable represents the cash balance of a trader.\nUnder the assumption that trading impacts stock prices, even zero-area cyclic\ntrading operations can induce geometric phases, i.e., profits or losses,\nwithout affecting the stock quote.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 17 Mar 2016 14:46:06 GMT"
            }
        ],
        "update_date": "2017-06-27",
        "authors_parsed": [
            [
                "Altafini",
                "Claudio",
                ""
            ]
        ]
    },
    {
        "id": "1312.7545",
        "submitter": "Peter Sarlin",
        "authors": "Peter Sarlin and Henrik J. Nyman",
        "title": "The process of macroprudential oversight in Europe",
        "comments": "Pre-print submitted for publication",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.GN",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The 2007--2008 financial crisis has paved the way for the use of\nmacroprudential policies in supervising the financial system as a whole. This\npaper views macroprudential oversight in Europe as a process, a sequence of\nactivities with the ultimate aim of safeguarding financial stability. To\nconceptualize a process in this context, we introduce the notion of a public\ncollaborative process (PCP). PCPs involve multiple organizations with a common\nobjective, where a number of dispersed organizations cooperate under various\nunstructured forms and take a collaborative approach to reaching the final\ngoal. We argue that PCPs can and should essentially be managed using the tools\nand practices common for business processes. To this end, we conduct an\nassessment of process readiness for macroprudential oversight in Europe. Based\nupon interviews with key European policymakers and supervisors, we provide an\nanalysis model to assess the maturity of five process enablers for\nmacroprudential oversight. With the results of our analysis, we give clear\nrecommendations on the areas that need further attention when macroprudential\noversight is being developed, in addition to providing a general purpose\nframework for monitoring the impact of improvement efforts.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 29 Dec 2013 15:01:45 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 9 Sep 2014 15:18:01 GMT"
            }
        ],
        "update_date": "2014-09-10",
        "authors_parsed": [
            [
                "Sarlin",
                "Peter",
                ""
            ],
            [
                "Nyman",
                "Henrik J.",
                ""
            ]
        ]
    },
    {
        "id": "2108.05747",
        "submitter": "Francisco Fern\\'andez Dr.",
        "authors": "Francisco M. Fern\\'andez",
        "title": "Comment on \"An appropriate approach to pricing european-style options\n  with the Adomian decomposition method\"",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.PR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that the Adomian decomposition method proposed by Ke et al [ANZIAM J.\n\\textbf{59} (2018) 349] is just the Taylor series approach in disguise. The\nlatter approach is simpler, more straightforward and yields a recurrence\nrelation free from integrals.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 12 Aug 2021 13:34:49 GMT"
            }
        ],
        "update_date": "2021-08-19",
        "authors_parsed": [
            [
                "Fern\u00e1ndez",
                "Francisco M.",
                ""
            ]
        ]
    },
    {
        "id": "2209.10498",
        "submitter": "Mandeep Singh Rai",
        "authors": "Mandeep Singh Rai",
        "title": "International institutions and power politics in the context of Chinese\n  Belt and Road Initiative",
        "comments": "11 Pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.GN q-fin.EC",
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  The subject of international institutions and power politics continues to\noccupy a central position in the field of International Relations and to the\nworld politics. It revolves around key questions on how rising states, regional\npowers and small states leverage international institutions for achieving\nsocial, political, economic gains for themselves. Taking into account one of\nthe rising powers China and the role of international institutions in the\ncontemporary international politics, this paper aims to demonstrate, how in\npursuit of power politics, various states (Small, Regional and Great powers)\nutilise international institutions by making them adapt to the new power\nrealities critical to world politics.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 21 Sep 2022 16:53:53 GMT"
            }
        ],
        "update_date": "2022-09-22",
        "authors_parsed": [
            [
                "Rai",
                "Mandeep Singh",
                ""
            ]
        ]
    },
    {
        "id": "2104.03794",
        "submitter": "Jens Peters F",
        "authors": "Claudia Tomasini Montenegro, Jens F. Peters, Manuel Baumann, Zhirong\n  Zhao-Karger, Christopher Wolter and Marcel Weil",
        "title": "Environmental assessment of a new generation battery: The\n  magnesium-sulfur system",
        "comments": "pre-print updated by revised version as accepted; 21 pages, 5\n  Figures, 1 table. Funded by the German Research Foundation (DFG) under\n  Project ID 390874152, the Initiative and Networking Fund of the Helmholtz\n  Association (ExNet-003) and the European Union's Horizon 2020 Research and\n  Innovation Programme under Grant Agreement No. 754382",
        "journal-ref": "Journal of Energy Storage Volume 35, March 2021, 102053. ISSN\n  2352-152X\n  (https://www.sciencedirect.com/science/article/pii/S2352152X20318879)",
        "doi": "10.1016/j.est.2020.102053",
        "report-no": null,
        "categories": "econ.GN q-fin.EC",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  As environmental concerns mostly drive the electrification of our economy and\nthe corresponding increase in demand for battery storage systems, information\nabout the potential environmental impacts of the different battery systems is\nrequired. However, this kind of information is scarce for emerging post-lithium\nsystems such as the magnesium-sulfur (MgS) battery. Therefore, we use life\ncycle assessment following a cradle-to-gate perspective to quantify the\ncumulative energy demand and potential environmental impacts per Wh of the\nstorage capacity of a hypothetical MgS battery (46 Wh/kg). Furthermore, we also\nestimate global warming potential (0.33 kg CO2 eq/Wh) , fossil depletion\npotential (0.09 kg oil eq / Wh), ozone depletion potential (2.5E-08 kg\nCFC-11/Wh) and metal depletion potential (0.044 kg Fe eq/Wh), associated with\nthe MgS battery production. The battery is modelled based on an existing\nprototype MgS pouch cell and hypothetically optimised according to the current\nstate of the art in lithium-ion batteries (LIB), exploring future improvement\npotentials. It turns out that the initial (non-optimised) prototype cell cannot\ncompete with current LIB in terms of energy density or environmental\nperformance, mainly due to the high share of non-active components, decreasing\nits performance substantially. Therefore, if the assumed evolutions of the MgS\ncell composition are achieved to overcome current design hurdles and reach a\ncomparable lifespan, efficiency, cost and safety levels to that of existing\nLIB; then the MgS battery has significant potential to outperform both existing\nLIB, and lithium-sulfur batteries.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 8 Apr 2021 14:18:49 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 11 Apr 2021 20:51:09 GMT"
            }
        ],
        "update_date": "2021-04-13",
        "authors_parsed": [
            [
                "Montenegro",
                "Claudia Tomasini",
                ""
            ],
            [
                "Peters",
                "Jens F.",
                ""
            ],
            [
                "Baumann",
                "Manuel",
                ""
            ],
            [
                "Zhao-Karger",
                "Zhirong",
                ""
            ],
            [
                "Wolter",
                "Christopher",
                ""
            ],
            [
                "Weil",
                "Marcel",
                ""
            ]
        ]
    },
    {
        "id": "1712.08329",
        "submitter": "Antoine Lejay",
        "authors": "Antoine Lejay (TOSCA, IECL), Paolo Pigato (WIAS)",
        "title": "A threshold model for local volatility: evidence of leverage and mean\n  reversion effects on historical data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.CP math.PR stat.AP",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In financial markets, low prices are generally associated with high\nvolatilities and vice-versa, this well known stylized fact usually being\nreferred to as leverage effect. We propose a local volatility model, given by a\nstochastic differential equation with piecewise constant coefficients, which\naccounts of leverage and mean-reversion effects in the dynamics of the prices.\nThis model exhibits a regime switch in the dynamics accordingly to a certain\nthreshold. It can be seen as a continuous-time version of the Self-Exciting\nThreshold Autoregressive (SETAR) model. We propose an estimation procedure for\nthe volatility and drift coefficients as well as for the threshold level.\nParameters estimated on the daily prices of 348 stocks of NYSE and S\\&P 500, on\ndifferent time windows, show consistent empirical evidence for leverageeffects.\nMean-reversion effects are also detected, most markedly in crisis periods.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 22 Dec 2017 07:42:13 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 23 Jan 2018 13:08:17 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 22 Oct 2018 06:32:15 GMT"
            },
            {
                "version": "v4",
                "created": "Fri, 22 Feb 2019 11:12:52 GMT"
            }
        ],
        "update_date": "2019-02-25",
        "authors_parsed": [
            [
                "Lejay",
                "Antoine",
                "",
                "TOSCA, IECL"
            ],
            [
                "Pigato",
                "Paolo",
                "",
                "WIAS"
            ]
        ]
    },
    {
        "id": "1211.6525",
        "submitter": "Shi-Ge Peng",
        "authors": "Shige Peng",
        "title": "The Pricing Mechanism of Contingent Claims and its Generating Function",
        "comments": "36 pages. arXiv admin note: substantial text overlap with\n  arXiv:math/0501415, arXiv:math/0605599",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.PR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study dynamic pricing mechanism of contingent claims. A\ntypical model of such pricing mechanism is the so-called g-expectation\n$E^g_{s,t}[X]$ defined by the solution of the backward stochastic differential\nequation with generator g and with the contingent claim X as terminal\ncondition. The generating function g this BSDE. We also provide examples of\ndetermining the price generating function $g=g(y,z)$ by testing.\n  The main result of this paper is as follows: if a given dynamic pricing\nmechanism is $E^{g_\\mu}$-dominated, i.e., the criteria (A5)\n$E_{s,t}[X]-E_{s,t}[X']\\leq E^{g_\\mu}_{s,t}[X-X']$ is satisfied for a large\nenough $\\mu> 0$, where $g_\\mu=g_{\\mu}(|y|+|z|)$, then $E_{s,t}$ is a g-pricing\nmechanism. This domination condition was statistically tested using CME data\ndocuments. The result of test is significantly positive.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 28 Nov 2012 05:46:19 GMT"
            }
        ],
        "update_date": "2012-11-29",
        "authors_parsed": [
            [
                "Peng",
                "Shige",
                ""
            ]
        ]
    },
    {
        "id": "2310.15512",
        "submitter": "Daniel Wilhelm",
        "authors": "Denis Chetverikov and Daniel Wilhelm",
        "title": "Inference for Rank-Rank Regressions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.EM math.ST stat.TH",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Slope coefficients in rank-rank regressions are popular measures of\nintergenerational mobility. In this paper, we first point out two important\nproperties of the OLS estimator in such regressions: commonly used variance\nestimators do not consistently estimate the asymptotic variance of the OLS\nestimator and, when the underlying distribution is not continuous, the OLS\nestimator may be highly sensitive to the way in which ties are handled.\nMotivated by these findings we derive the asymptotic theory for the OLS\nestimator in a general rank-rank regression specification without making\nassumptions about the continuity of the underlying distribution. We then extend\nthe asymptotic theory to other regressions involving ranks that have been used\nin empirical work. Finally, we apply our new inference methods to three\nempirical studies. We find that the confidence intervals based on estimators of\nthe correct variance may sometimes be substantially shorter and sometimes\nsubstantially longer than those based on commonly used variance estimators. The\ndifferences in confidence intervals concern economically meaningful values of\nmobility and thus may lead to different conclusions when comparing mobility\nacross different regions or countries.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Oct 2023 04:41:37 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 23 May 2024 07:51:54 GMT"
            }
        ],
        "update_date": "2024-05-24",
        "authors_parsed": [
            [
                "Chetverikov",
                "Denis",
                ""
            ],
            [
                "Wilhelm",
                "Daniel",
                ""
            ]
        ]
    },
    {
        "id": "2211.07416",
        "submitter": "Simon Weber",
        "authors": "Simon Weber",
        "title": "Collective models and the marriage market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.GN q-fin.EC",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper, I develop an integrated approach to collective models and\nmatching models of the marriage market. In the collective framework, both\nhousehold formation and the intra-household allocation of bargaining power are\ntaken as given. This is no longer the case in the present contribution, where\nboth are endogenous to the determination of equilibrium on the marriage market.\nI characterize a class of \"proper\" collective models which can be embedded into\na general matching framework with imperfectly transferable utility. In such\nmodels, the bargaining sets are parametrized by an analytical device called\ndistance function, which plays a key role both for writing down the usual\nstability conditions and for estimation. In general, however, distance\nfunctions are not known in closed-form. I provide an efficient method for\ncomputing distance functions, that works even with the most complex collective\nmodels. Finally, I provide a fully-fledged application using PSID data. I\nidentify the sharing rule and its distribution and study the evolution of the\nsharing rule and housework time sharing in the United States since 1969. In a\ncounterfactual experiment, I simulate the impact of closing the gender wage\ngap.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 14 Nov 2022 14:42:31 GMT"
            }
        ],
        "update_date": "2022-11-15",
        "authors_parsed": [
            [
                "Weber",
                "Simon",
                ""
            ]
        ]
    },
    {
        "id": "1501.04682",
        "submitter": "Peter Sarlin",
        "authors": "Markus Holopainen, Peter Sarlin",
        "title": "Toward robust early-warning models: A horse race, ensembles and model\n  uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.ST cs.CE q-fin.CP q-fin.EC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents first steps toward robust models for crisis prediction.\nWe conduct a horse race of conventional statistical methods and more recent\nmachine learning methods as early-warning models. As individual models are in\nthe literature most often built in isolation of other methods, the exercise is\nof high relevance for assessing the relative performance of a wide variety of\nmethods. Further, we test various ensemble approaches to aggregating the\ninformation products of the built models, providing a more robust basis for\nmeasuring country-level vulnerabilities. Finally, we provide approaches to\nestimating model uncertainty in early-warning exercises, particularly model\nperformance uncertainty and model output uncertainty. The approaches put\nforward in this paper are shown with Europe as a playground. Generally, our\nresults show that the conventional statistical approaches are outperformed by\nmore advanced machine learning methods, such as k-nearest neighbors and neural\nnetworks, and particularly by model aggregation approaches through ensemble\nlearning.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 20 Jan 2015 00:18:18 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 4 Feb 2015 15:55:18 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 1 Apr 2016 14:22:36 GMT"
            }
        ],
        "update_date": "2016-04-04",
        "authors_parsed": [
            [
                "Holopainen",
                "Markus",
                ""
            ],
            [
                "Sarlin",
                "Peter",
                ""
            ]
        ]
    },
    {
        "id": "2301.05999",
        "submitter": "Gaurab Aryal",
        "authors": "Gaurab Aryal and Dennis J. Campbell and Federico Ciliberto and\n  Ekaterina A. Khmelnitskaya",
        "title": "Common Subcontracting and Airline Prices",
        "comments": "forthcoming in The Review of Economics and Statistics",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.GN q-fin.EC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the US airline industry, independent regional airlines fly passengers on\nbehalf of several national airlines across different markets, giving rise to\n$\\textit{common subcontracting}$. On the one hand, we find that subcontracting\nis associated with lower prices, consistent with the notion that regional\nairlines tend to fly passengers at lower costs than major airlines. On the\nother hand, we find that $\\textit{common}$ subcontracting is associated with\nhigher prices. These two countervailing effects suggest that the growth of\nregional airlines can have anticompetitive implications for the industry.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 15 Jan 2023 02:09:45 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 25 Sep 2023 19:44:35 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 13 Nov 2023 16:54:01 GMT"
            },
            {
                "version": "v4",
                "created": "Sat, 23 Dec 2023 07:33:41 GMT"
            }
        ],
        "update_date": "2023-12-27",
        "authors_parsed": [
            [
                "Aryal",
                "Gaurab",
                ""
            ],
            [
                "Campbell",
                "Dennis J.",
                ""
            ],
            [
                "Ciliberto",
                "Federico",
                ""
            ],
            [
                "Khmelnitskaya",
                "Ekaterina A.",
                ""
            ]
        ]
    },
    {
        "id": "2405.01341",
        "submitter": "Paolo Pin",
        "authors": "Ugo Bolletta, Paolo Pin",
        "title": "Dynamic opinion updating with endogenous networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.TH",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Polarization is a well-documented phenomenon across a wide range of social\nissues. However, prevailing theories often compartmentalize the examination of\nherding behavior and opinion convergence within different contexts. In this\nstudy, we delve into the micro-foundations of how individuals strategically\nselect reference groups, offering insight into a dynamic process where both\nindividual opinions and the network evolve simultaneously. We base our model on\ntwo parameters: people's direct benefit from connections and their adaptability\nin adjusting their opinions. Our research highlights which conditions impede\nthe network from achieving complete connectivity, resulting in enduring\npolarization. Notably, our model also reveals that polarization can transiently\nemerge during the transition towards consensus. We explore the connection\nbetween these scenarios and a critical network metric: the initial diameter,\nunder specific conditions related to the initial distribution of opinions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 2 May 2024 14:45:04 GMT"
            }
        ],
        "update_date": "2024-05-03",
        "authors_parsed": [
            [
                "Bolletta",
                "Ugo",
                ""
            ],
            [
                "Pin",
                "Paolo",
                ""
            ]
        ]
    },
    {
        "id": "1105.1488",
        "submitter": "Nikolai Dokuchaev",
        "authors": "Nikolai Dokuchaev",
        "title": "The structure of optimal portfolio strategies for continuous time\n  markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.PM cs.SY math.OC math.PR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper studies problem of continuous time optimal portfolio selection for\na incom- plete market diffusion model. It is shown that, under some mild\nconditions, near optimal strategies for investors with different performance\ncriteria can be constructed using a limited number of fixed processes (mutual\nfunds), for a market with a larger number of available risky stocks. In other\nwords, a dimension reduction is achieved via a relaxed version of the Mutual\nFund Theorem.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 8 May 2011 03:21:43 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 9 Dec 2013 12:35:35 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 12 Dec 2013 14:33:48 GMT"
            },
            {
                "version": "v4",
                "created": "Mon, 31 Mar 2014 12:29:57 GMT"
            },
            {
                "version": "v5",
                "created": "Mon, 14 Apr 2014 12:37:18 GMT"
            }
        ],
        "update_date": "2014-04-15",
        "authors_parsed": [
            [
                "Dokuchaev",
                "Nikolai",
                ""
            ]
        ]
    },
    {
        "id": "2206.01064",
        "submitter": "Man Yiu Tsang",
        "authors": "Man Yiu Tsang, Tony Sit, Hoi Ying Wong",
        "title": "Adaptive Robust Online Portfolio Selection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.PM math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The online portfolio selection (OLPS) problem differs from classical\nportfolio model problems, as it involves making sequential investment\ndecisions. Many OLPS strategies described in the literature capture market\nmovement based on various beliefs and are shown to be profitable. In this\npaper, we propose a robust optimization (RO)-based strategy that takes\ntransaction costs into account. Moreover, unlike existing studies that\ncalibrate model parameters from benchmark data sets, we develop a novel\nadaptive scheme that decides the parameters sequentially. With a wide range of\nparameters as input, our scheme captures market uptrend and protects against\nmarket downtrend while controlling trading frequency to avoid excessive\ntransaction costs. We numerically demonstrate the advantages of our adaptive\nscheme against several benchmarks under various settings. Our adaptive scheme\nmay also be useful in general sequential decision-making problems. Finally, we\ncompare the performance of our strategy with that of existing OLPS strategies\nusing both benchmark and newly collected data sets. Our strategy outperforms\nthese existing OLPS strategies in terms of cumulative returns and competitive\nSharpe ratios across diversified data sets, demonstrating its\nadaptability-driven superiority.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 2 Jun 2022 14:29:46 GMT"
            }
        ],
        "update_date": "2022-06-03",
        "authors_parsed": [
            [
                "Tsang",
                "Man Yiu",
                ""
            ],
            [
                "Sit",
                "Tony",
                ""
            ],
            [
                "Wong",
                "Hoi Ying",
                ""
            ]
        ]
    },
    {
        "id": "1309.5703",
        "submitter": "Magomet Yandiev Mr",
        "authors": "Magomet Yandiev, Alexander Pakhalov",
        "title": "The Relationship Between Stock Market Parameters and Interbank Lending\n  Market: an Empirical Evidence",
        "comments": "15 pages, 11 appendixes",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.ST",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The article presents calculations that prove practical importance of the\nearlier derived theoretical relationship between the interest rate on the\ninterbank credit market, volume of investment and the quantity of securities\ntradable on the stock exchange.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 23 Sep 2013 06:27:54 GMT"
            }
        ],
        "update_date": "2013-09-24",
        "authors_parsed": [
            [
                "Yandiev",
                "Magomet",
                ""
            ],
            [
                "Pakhalov",
                "Alexander",
                ""
            ]
        ]
    },
    {
        "id": "2004.13347",
        "submitter": "Kei Nakagawa Ph.D",
        "authors": "Kei Nakagawa, Shuhei Noma, Masaya Abe",
        "title": "RM-CVaR: Regularized Multiple $\\beta$-CVaR Portfolio",
        "comments": "accepted by the IJCAI-PRICAI 2020 Special Track AI in FinTech",
        "journal-ref": null,
        "doi": "10.24963/ijcai.2020/629",
        "report-no": null,
        "categories": "q-fin.PM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The problem of finding the optimal portfolio for investors is called the\nportfolio optimization problem. Such problem mainly concerns the expectation\nand variability of return (i.e., mean and variance). Although the variance\nwould be the most fundamental risk measure to be minimized, it has several\ndrawbacks. Conditional Value-at-Risk (CVaR) is a relatively new risk measure\nthat addresses some of the shortcomings of well-known variance-related risk\nmeasures, and because of its computational efficiencies, it has gained\npopularity. CVaR is defined as the expected value of the loss that occurs\nbeyond a certain probability level ($\\beta$). However, portfolio optimization\nproblems that use CVaR as a risk measure are formulated with a single $\\beta$\nand may output significantly different portfolios depending on how the $\\beta$\nis selected. We confirm even small changes in $\\beta$ can result in huge\nchanges in the whole portfolio structure. In order to improve this problem, we\npropose RM-CVaR: Regularized Multiple $\\beta$-CVaR Portfolio. We perform\nexperiments on well-known benchmarks to evaluate the proposed portfolio.\nCompared with various portfolios, RM-CVaR demonstrates a superior performance\nof having both higher risk-adjusted returns and lower maximum drawdown.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 28 Apr 2020 07:49:06 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 9 May 2020 05:07:01 GMT"
            }
        ],
        "update_date": "2020-07-21",
        "authors_parsed": [
            [
                "Nakagawa",
                "Kei",
                ""
            ],
            [
                "Noma",
                "Shuhei",
                ""
            ],
            [
                "Abe",
                "Masaya",
                ""
            ]
        ]
    },
    {
        "id": "2309.11690",
        "submitter": "Tamay Besiroglu",
        "authors": "Ege Erdil, Tamay Besiroglu",
        "title": "Explosive growth from AI automation: A review of the arguments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.GN q-fin.EC",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We examine whether substantial AI automation could accelerate global economic\ngrowth by about an order of magnitude, akin to the economic growth effects of\nthe Industrial Revolution. We identify three primary drivers for such growth:\n1) the scalability of an AI ``labor force\" restoring a regime of increasing\nreturns to scale, 2) the rapid expansion of an AI labor force, and 3) a massive\nincrease in output from rapid automation occurring over a brief period of time.\nAgainst this backdrop, we evaluate nine counterarguments, including regulatory\nhurdles, production bottlenecks, alignment issues, and the pace of automation.\nWe tentatively assess these arguments, finding most are unlikely deciders. We\nconclude that explosive growth seems plausible with AI capable of broadly\nsubstituting for human labor, but high confidence in this claim seems currently\nunwarranted. Key questions remain about the intensity of regulatory responses\nto AI, physical bottlenecks in production, the economic value of superhuman\nabilities, and the rate at which AI automation could occur.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 20 Sep 2023 23:45:14 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 1 Oct 2023 13:50:04 GMT"
            }
        ],
        "update_date": "2023-10-03",
        "authors_parsed": [
            [
                "Erdil",
                "Ege",
                ""
            ],
            [
                "Besiroglu",
                "Tamay",
                ""
            ]
        ]
    },
    {
        "id": "2008.09529",
        "submitter": "Ali Shourideh",
        "authors": "Maryam Saeedi, Ali Shourideh",
        "title": "Optimal Rating Design under Moral Hazard",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.TH",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine the design of optimal rating systems in the presence of moral\nhazard. First, an intermediary commits to a rating scheme. Then, a\ndecision-maker chooses an action that generates value for the buyer. The\nintermediary then observes a noisy signal of the decision-maker's choice and\nsends the buyer a signal consistent with the rating scheme. Here we fully\ncharacterize the set of allocations that can arise in equilibrium under any\narbitrary rating system. We use this characterization to study various design\naspects of optimal rating systems. Specifically, we study the properties of\noptimal ratings when the decision-maker's effort is productive and when the\ndecision-maker can manipulate the intermediary's signal with a noise. With\nmanipulation, rating uncertainty is a fairly robust feature of optimal rating\nsystems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 21 Aug 2020 15:11:22 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 7 Sep 2020 16:13:48 GMT"
            },
            {
                "version": "v3",
                "created": "Sun, 23 Jul 2023 23:25:40 GMT"
            }
        ],
        "update_date": "2023-07-25",
        "authors_parsed": [
            [
                "Saeedi",
                "Maryam",
                ""
            ],
            [
                "Shourideh",
                "Ali",
                ""
            ]
        ]
    },
    {
        "id": "1102.2285",
        "submitter": "Qingshuo Song",
        "authors": "Qingshuo Song",
        "title": "Approximating Functional of Local Martingale Under the Lack of\n  Uniqueness of Black-Scholes PDE",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.PR math.PR q-fin.CP",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When the underlying stock price is a strict local martingale process under an\nequivalent local martingale measure, Black-Scholes PDE associated with an\nEuropean option may have multiple solutions. In this paper, we study an\napproximation for the smallest hedging price of such an European option. Our\nresults show that a class of rebate barrier options can be used for this\napproximation. Among of them, a specific rebate option is also provided with a\ncontinuous rebate function, which corresponds to the unique classical solution\nof the associated parabolic PDE. Such a construction makes existing numerical\nPDE techniques applicable for its computation. An asymptotic convergence rate\nis also studied when the knocked-out barrier moves to infinity under suitable\nconditions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 11 Feb 2011 05:19:42 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 8 Oct 2011 04:50:11 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 21 Sep 2012 13:40:14 GMT"
            }
        ],
        "update_date": "2012-09-24",
        "authors_parsed": [
            [
                "Song",
                "Qingshuo",
                ""
            ]
        ]
    },
    {
        "id": "2101.02736",
        "submitter": "Wei Dai",
        "authors": "Yong Shi, Wei Dai, Wen Long, Bo Li",
        "title": "Improved ACD-based financial trade durations prediction leveraging LSTM\n  networks and Attention Mechanism",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.CP",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The liquidity risk factor of security market plays an important role in the\nformulation of trading strategies. A more liquid stock market means that the\nsecurities can be bought or sold more easily. As a sound indicator of market\nliquidity, the transaction duration is the focus of this study. We concentrate\non estimating the probability density function p({\\Delta}t_(i+1) |G_i) where\n{\\Delta}t_(i+1) represents the duration of the (i+1)-th transaction, G_i\nrepresents the historical information at the time when the (i+1)-th transaction\noccurs. In this paper, we propose a new ultra-high-frequency (UHF) duration\nmodelling framework by utilizing long short-term memory (LSTM) networks to\nextend the conditional mean equation of classic autoregressive conditional\nduration (ACD) model while retaining the probabilistic inference ability. And\nthen the attention mechanism is leveraged to unveil the internal mechanism of\nthe constructed model. In order to minimize the impact of manual parameter\ntuning, we adopt fixed hyperparameters during the training process. The\nexperiments applied to a large-scale dataset prove the superiority of the\nproposed hybrid models. In the input sequence, the temporal positions which are\nmore important for predicting the next duration can be efficiently highlighted\nvia the added attention mechanism layer.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 7 Jan 2021 19:42:21 GMT"
            }
        ],
        "update_date": "2021-01-11",
        "authors_parsed": [
            [
                "Shi",
                "Yong",
                ""
            ],
            [
                "Dai",
                "Wei",
                ""
            ],
            [
                "Long",
                "Wen",
                ""
            ],
            [
                "Li",
                "Bo",
                ""
            ]
        ]
    },
    {
        "id": "2312.02943",
        "submitter": "Shihao Zhu",
        "authors": "An Chen, Giorgio Ferrari, Shihao Zhu",
        "title": "Striking the Balance: Life Insurance Timing and Asset Allocation in\n  Financial Planning",
        "comments": "37 pages, 8 figures. arXiv admin note: text overlap with\n  arXiv:2212.05317",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.PM",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates the consumption and investment decisions of an\nindividual facing uncertain lifespan and stochastic labor income within a\nBlack-Scholes market framework. A key aspect of our study involves the agent's\noption to choose when to acquire life insurance for bequest purposes. We\nexamine two scenarios: one with a fixed bequest amount and another with a\ncontrolled bequest amount. Applying duality theory and addressing free-boundary\nproblems, we analytically solve both cases, and provide explicit expressions\nfor value functions and optimal strategies in both cases. In the first\nscenario, where the bequest amount is fixed, distinct outcomes emerge based on\ndifferent levels of risk aversion parameter $\\gamma$: (i) the optimal time for\nlife insurance purchase occurs when the agent's wealth surpasses a critical\nthreshold if $\\gamma \\in (0,1)$, or (ii) life insurance should be acquired\nimmediately if $\\gamma>1$. In contrast, in the second scenario with a\ncontrolled bequest amount, regardless of $\\gamma$ values, immediate life\ninsurance purchase proves to be optimal.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 5 Dec 2023 18:10:31 GMT"
            }
        ],
        "update_date": "2023-12-24",
        "authors_parsed": [
            [
                "Chen",
                "An",
                ""
            ],
            [
                "Ferrari",
                "Giorgio",
                ""
            ],
            [
                "Zhu",
                "Shihao",
                ""
            ]
        ]
    },
    {
        "id": "2203.07145",
        "submitter": "Jonas Crevecoeur",
        "authors": "Jonas Crevecoeur, Katrien Antonio, Stijn Desmedt, Alexandre Masquelein",
        "title": "Bridging the gap between pricing and reserving with an occurrence and\n  development model for non-life insurance claims",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.RM",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Due to the presence of reporting and settlement delay, claim data sets\ncollected by non-life insurance companies are typically incomplete, facing\nright censored claim count and claim severity observations. Current practice in\nnon-life insurance pricing tackles these right censored data via a two-step\nprocedure. First, best estimates are computed for the number of claims that\noccurred in past exposure periods and the ultimate claim severities, using the\nincomplete, historical claim data. Second, pricing actuaries build predictive\nmodels to estimate technical, pure premiums for new contracts by treating these\nbest estimates as actual observed outcomes, hereby neglecting their inherent\nuncertainty. We propose an alternative approach that brings valuable insights\nfor both non-life pricing as well as reserving. As such we effectively bridge\nthese two key actuarial tasks that have traditionally been discussed in silos.\nHereto we develop a granular occurrence and development model for non-life\nclaims that tackles reserving and at the same time resolves the inconsistency\nin traditional pricing techniques between actual observations and imputed best\nestimates. We illustrate our proposed model on an insurance as well as a\nreinsurance portfolio. The advantages of our proposed strategy are most\ncompelling in the reinsurance illustration where large uncertainties in the\nbest estimates originate from long reporting and settlement delays, low claim\nfrequencies and heavy (even extreme) claim sizes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 14 Mar 2022 14:40:36 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 9 Feb 2023 16:22:15 GMT"
            }
        ],
        "update_date": "2023-02-10",
        "authors_parsed": [
            [
                "Crevecoeur",
                "Jonas",
                ""
            ],
            [
                "Antonio",
                "Katrien",
                ""
            ],
            [
                "Desmedt",
                "Stijn",
                ""
            ],
            [
                "Masquelein",
                "Alexandre",
                ""
            ]
        ]
    },
    {
        "id": "1302.3870",
        "submitter": "Tomoyuki Ichiba",
        "authors": "Robert Fernholz, Tomoyuki Ichiba and Ioannis Karatzas",
        "title": "A second-order stock market model",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.ST math.PR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A first-order model for a stock market assigns to each stock a return\nparameter and a variance parameter that depend only on the rank of the stock. A\nsecond-order model assigns these parameters based on both the rank and the name\nof the stock. First- and second-order models exhibit stability properties that\nmake them appropriate as a backdrop for the analysis of the idiosyncratic\nbehavior of individual stocks. Methods for the estimation of the parameters of\nsecond-order models are developed in this paper.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 15 Feb 2013 20:33:13 GMT"
            }
        ],
        "update_date": "2013-02-18",
        "authors_parsed": [
            [
                "Fernholz",
                "Robert",
                ""
            ],
            [
                "Ichiba",
                "Tomoyuki",
                ""
            ],
            [
                "Karatzas",
                "Ioannis",
                ""
            ]
        ]
    },
    {
        "id": "2301.08136",
        "submitter": "Nizar Riane",
        "authors": "Nizar Riane, Claire David",
        "title": "Input-Output Analysis: New Results From Markov Chain Theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.GN q-fin.EC",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this work, we propose a new lecture of input-output model reconciliation\nMarkov chain and the dominance theory, in the field of interindustrial poles\ninteractions. A deeper lecture of Leontieff table in term of Markov chain is\ngiven, exploiting spectral properties and time to absorption to characterize\nproduction processes, then the dualities local-global/dominance- Sensitivity\nanalysis are established, allowing a better understanding of economic poles\narrangement. An application to the Moroccan economy is given.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 13 Jan 2023 18:49:50 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 22 Jan 2023 14:17:31 GMT"
            }
        ],
        "update_date": "2023-01-24",
        "authors_parsed": [
            [
                "Riane",
                "Nizar",
                ""
            ],
            [
                "David",
                "Claire",
                ""
            ]
        ]
    },
    {
        "id": "1204.2736",
        "submitter": "Alfonsi Aurelien",
        "authors": "Aur\\'elien Alfonsi (CERMICS), Jos\\'e Infante Acevedo (CERMICS)",
        "title": "Optimal execution and price manipulations in time-varying limit order\n  books",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.TR math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper focuses on an extension of the Limit Order Book (LOB) model with\ngeneral shape introduced by Alfonsi, Fruth and Schied. Here, the additional\nfeature allows a time-varying LOB depth. We solve the optimal execution problem\nin this framework for both discrete and continuous time strategies. This gives\nin particular sufficient conditions to exclude Price Manipulations in the sense\nof Huberman and Stanzl or Transaction-Triggered Price Manipulations (see\nAlfonsi, Schied and Slynko). These conditions give interesting qualitative\ninsights on how market makers may create or not price manipulations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 12 Apr 2012 14:27:07 GMT"
            }
        ],
        "update_date": "2012-04-16",
        "authors_parsed": [
            [
                "Alfonsi",
                "Aur\u00e9lien",
                "",
                "CERMICS"
            ],
            [
                "Acevedo",
                "Jos\u00e9 Infante",
                "",
                "CERMICS"
            ]
        ]
    },
    {
        "id": "1805.06345",
        "submitter": "Henryk Gzyl",
        "authors": "Henryk Gzyl and Alfredo Rios",
        "title": "Which portfolio is better? A discussion of several possible comparison\n  criteria",
        "comments": "Discovered some wrong statements in it. Will be eventually replaced\n  when corrected",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.PM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  During the last few years, there has been an interest in comparing simple or\nheuristic procedures for portfolio selection, such as the naive, equal weights,\nportfolio choice, against more \"sophisticated\" portfolio choices, and in\nexplaining why, in some cases, the heuristic choice seems to outperform the\nsophisticated choice. We believe that some of these results may be due to the\ncomparison criterion used. It is the purpose of this note to analyze some ways\nof comparing the performance of portfolios. We begin by analyzing each\ncriterion proposed on the market line, in which there is only one random\nreturn. Several possible comparisons between optimal portfolios and the naive\nportfolio are possible and easy to establish. Afterwards, we study the case in\nwhich there is no risk free asset. In this way, we believe some basic\ntheoretical questions regarding why some portfolios may seem to outperform\nothers can be clarified.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 16 May 2018 14:31:26 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 18 Oct 2018 22:44:25 GMT"
            },
            {
                "version": "v3",
                "created": "Sat, 4 Jun 2022 15:04:20 GMT"
            }
        ],
        "update_date": "2022-06-07",
        "authors_parsed": [
            [
                "Gzyl",
                "Henryk",
                ""
            ],
            [
                "Rios",
                "Alfredo",
                ""
            ]
        ]
    },
    {
        "id": "1211.3102",
        "submitter": "Tim Garrett",
        "authors": "Timothy J. Garrett",
        "title": "Can we predict long-run economic growth?",
        "comments": null,
        "journal-ref": "Garrett, T. J., 2012: Can we predict long-run economic growth?,\n  Retirement Management Journal 2(2) 53-61",
        "doi": null,
        "report-no": null,
        "categories": "q-fin.GN physics.soc-ph",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For those concerned with the long-term value of their accounts, it can be a\nchallenge to plan in the present for inflation-adjusted economic growth over\ncoming decades. Here, I argue that there exists an economic constant that\ncarries through time, and that this can help us to anticipate the more distant\nfuture: global economic wealth has a fixed link to civilization's overall rate\nof energy consumption from all sources; the ratio of these two quantities has\nnot changed over the past 40 years that statistics are available. Power\nproduction and wealth rise equally quickly because civilization, like any other\nsystem in the universe, must consume and dissipate its energy reserves in order\nto sustain its current size. One perspective might be that financial wealth\nmust ultimately collapse as we deplete our energy reserves. However, we can\nalso expect that highly aggregated quantities like global wealth have inertia,\nand that growth rates must persist. Exceptionally rapid innovation in the two\ndecades following 1950 allowed for unprecedented acceleration of\ninflation-adjusted rates of return. But today, real innovation rates are more\nstagnant. This means that, over the coming decade or so, global GDP and wealth\nshould rise fairly steadily at an inflation-adjusted rate of about 2.2% per\nyear.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 13 Nov 2012 20:39:10 GMT"
            }
        ],
        "update_date": "2012-11-14",
        "authors_parsed": [
            [
                "Garrett",
                "Timothy J.",
                ""
            ]
        ]
    },
    {
        "id": "1602.06213",
        "submitter": "Li-Xin Wang",
        "authors": "Li-Xin Wang",
        "title": "Modeling Stock Price Dynamics with Fuzzy Opinion Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.TR cs.SI cs.SY q-fin.CP",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a mathematical model for the word-of-mouth communications among\nstock investors through social networks and explore how the changes of the\ninvestors' social networks influence the stock price dynamics and vice versa.\nAn investor is modeled as a Gaussian fuzzy set (a fuzzy opinion) with the\ncenter and standard deviation as inputs and the fuzzy set itself as output.\nInvestors are connected in the following fashion: the center input of an\ninvestor is taken as the average of the neighbors' outputs, where two investors\nare neighbors if their fuzzy opinions are close enough to each other, and the\nstandard deviation (uncertainty) input is taken with local, global or external\nreference schemes to model different scenarios of how investors define\nuncertainties. The centers and standard deviations of the fuzzy opinions are\nthe expected prices and their uncertainties, respectively, that are used as\ninputs to the price dynamic equation. We prove that with the local reference\nscheme the investors converge to different groups in finite time, while with\nthe global or external reference schemes all investors converge to a consensus\nwithin finite time and the consensus may change with time in the external\nreference case. We show how to model trend followers, contrarians and\nmanipulators within this mathematical framework and prove that the biggest\nenemy of a manipulator is the other manipulators. We perform Monte Carlo\nsimulations to show how the model parameters influence the price dynamics, and\nwe apply a modified version of the model to the daily closing prices of fifteen\ntop banking and real estate stocks in Hong Kong for the recent two years from\nDec. 5, 2013 to Dec. 4, 2015 and discover that a sharp increase of the combined\nuncertainty is a reliable signal to predict the reversal of the current price\ntrend.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 19 Feb 2016 16:35:05 GMT"
            }
        ],
        "update_date": "2016-02-22",
        "authors_parsed": [
            [
                "Wang",
                "Li-Xin",
                ""
            ]
        ]
    },
    {
        "id": "2211.03426",
        "submitter": "Michele Crescenzi",
        "authors": "Michele Crescenzi",
        "title": "Coordination through ambiguous language",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.TH",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a syntactic construction of correlated equilibrium. For any finite\ngame, we study how players coordinate their play on a signal by means of a\npublic strategy whose instructions are expressed in some natural language.\nLanguage can be ambiguous in that different players may assign different truth\nvalues to the very same formula in the same state of the world. We model\nambiguity using the player-dependent logic of Halpern and Kets (2015). We show\nthat, absent any ambiguity, self-enforcing coordination always induces a\ncorrelated equilibrium of the underlying game. When language ambiguity is\nallowed, self-enforcing coordination strategies induce subjective correlated\nequilibria.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 7 Nov 2022 10:31:04 GMT"
            }
        ],
        "update_date": "2022-11-08",
        "authors_parsed": [
            [
                "Crescenzi",
                "Michele",
                ""
            ]
        ]
    },
    {
        "id": "2109.03541",
        "submitter": "Jiamin Yu",
        "authors": "Jiamin Yu",
        "title": "Three fundamental problems in risk modeling on big data: an information\n  theory view",
        "comments": "6 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.RM q-fin.MF",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Since Claude Shannon founded Information Theory, information theory has\nwidely fostered other scientific fields, such as statistics, artificial\nintelligence, biology, behavioral science, neuroscience, economics, and\nfinance. Unfortunately, actuarial science has hardly benefited from information\ntheory. So far, only one actuarial paper on information theory can be searched\nby academic search engines. Undoubtedly, information and risk, both as\nUncertainty, are constrained by entropy law. Today's insurance big data era\nmeans more data and more information. It is unacceptable for risk management\nand actuarial science to ignore information theory. Therefore, this paper aims\nto exploit information theory to discover the performance limits of insurance\nbig data systems and seek guidance for risk modeling and the development of\nactuarial pricing systems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 8 Sep 2021 10:58:06 GMT"
            }
        ],
        "update_date": "2021-09-09",
        "authors_parsed": [
            [
                "Yu",
                "Jiamin",
                ""
            ]
        ]
    },
    {
        "id": "2304.12450",
        "submitter": "Carsten Chong",
        "authors": "Carsten H. Chong, Viktor Todorov",
        "title": "Asymptotic Expansions for High-Frequency Option Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.ST math.PR q-fin.MF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive a nonparametric higher-order asymptotic expansion for small-time\nchanges of conditional characteristic functions of It\\^o semimartingale\nincrements. The asymptotics setup is of joint type: both the length of the time\ninterval of the increment of the underlying process and the time gap between\nevaluating the conditional characteristic function are shrinking. The spot\nsemimartingale characteristics of the underlying process as well as their spot\nsemimartingale characteristics appear as leading terms in the derived\nasymptotic expansions. The analysis applies to a general class of It\\^o\nsemimartingales that includes in particular L\\'evy-driven SDEs and time-changed\nL\\'evy processes. The asymptotic expansion results are of direct use for\nconstructing nonparametric estimates pertaining to the stochastic volatility\ndynamics of an asset from high-frequency data of options written on the\nunderlying asset.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 24 Apr 2023 21:03:12 GMT"
            }
        ],
        "update_date": "2023-04-26",
        "authors_parsed": [
            [
                "Chong",
                "Carsten H.",
                ""
            ],
            [
                "Todorov",
                "Viktor",
                ""
            ]
        ]
    },
    {
        "id": "2301.10675",
        "submitter": "Robert Seamans",
        "authors": "Qiren Liu, Sen Luo and Robert Seamans",
        "title": "Pain or Anxiety? The Health Consequences of Rising Robot Adoption in\n  China",
        "comments": "22 pages, 8 tables",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.GN q-fin.EC",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The rising adoption of industrial robots is radically changing the role of\nworkers in the production process. Robots can be used for some of the more\nphysically demanding and dangerous production work, thus reducing the\npossibility of worker injury. On the other hand, robots may replace workers,\npotentially increasing worker anxiety about their job safety. In this paper, we\ninvestigate how individual physical health and mental health outcomes vary with\nlocal exposure to robots for manufacturing workers in China. We find a link\nbetween robot exposure and better physical health of workers, particularly for\nyounger workers and those with less education. However, we also find that robot\nexposure is associated with more mental stress for Chinese workers,\nparticularly older and less educated workers.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Jan 2023 16:27:02 GMT"
            }
        ],
        "update_date": "2023-01-26",
        "authors_parsed": [
            [
                "Liu",
                "Qiren",
                ""
            ],
            [
                "Luo",
                "Sen",
                ""
            ],
            [
                "Seamans",
                "Robert",
                ""
            ]
        ]
    },
    {
        "id": "2211.11968",
        "submitter": "arXiv Admin",
        "authors": "Zhu Xiaoxu and Fan kecai and He hai and Zhang Ziyu",
        "title": "Birth Order and Son Preference to Determine the Children of Shandong\n  Province So Tall",
        "comments": "arXiv admin note: This submission has been withdrawn by arXiv\n  administrators due to inappropriate text overlap with external sources",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.GN q-fin.EC",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  More children in Shandong Province are stunted than any other province in\nChina. Data on more than 122,000 children show a dramatic increase in height\nadvantage with birth order in Shandong relative to the average of other\nprovinces. We suggest that the steep birth order gradient in Shandong is due to\na preference for the eldest child, which influences parental fertility\ndecisions and resource allocation to children. We show that within Shandong\nprovince, the gradient is steeper for regions and cultures with a high\npreference for the eldest child. As predicted, this gradient also varies with\nthe sex of the sibling. By back-calculating, the steeper birth order gradient\nin Shandong Province explains more than half of the average height gap between\nShandong Province and the rest of China.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 22 Nov 2022 02:59:56 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 28 Nov 2022 16:40:42 GMT"
            }
        ],
        "update_date": "2022-11-29",
        "authors_parsed": [
            [
                "Xiaoxu",
                "Zhu",
                ""
            ],
            [
                "kecai",
                "Fan",
                ""
            ],
            [
                "hai",
                "He",
                ""
            ],
            [
                "Ziyu",
                "Zhang",
                ""
            ]
        ]
    },
    {
        "id": "1511.07359",
        "submitter": "Jean-Philippe Bouchaud",
        "authors": "A. Rej, R. Benichou, J. de Lataillade, G. Z\\'erah, J.-Ph. Bouchaud",
        "title": "Optimal Trading with Linear and (small) Non-Linear Costs",
        "comments": "16 pages, 1 figure, paragraph added to explain the relation of our\n  results with arXiv:1402.5306",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.TR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We reconsider the problem of optimal trading in the presence of linear and\nquadratic costs, for arbitrary linear costs but in the limit where quadratic\ncosts are small. Using matched asymptotic expansion techniques, we find that\nthe trading speed vanishes inside a band that is narrower than in the absence\nof quadratic costs, by an amount that scales as the one-third power of\nquadratic costs. Outside of the band, we find three regimes: a small boundary\nlayer where the velocity vanishes linearly with the distance to the band, an\nintermediate region where the velocity behaves as a square-root of that\ndistance, and a far region where it becomes linear. Our solution is consistent\nwith available numerical results. We determine the conditions in which our\nexpansion is useful in practical applications, and generalize our solution to\nother forms of non-linear costs.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 23 Nov 2015 18:46:05 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 9 Jan 2016 09:37:19 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 14 Nov 2016 17:54:26 GMT"
            }
        ],
        "update_date": "2016-11-15",
        "authors_parsed": [
            [
                "Rej",
                "A.",
                ""
            ],
            [
                "Benichou",
                "R.",
                ""
            ],
            [
                "de Lataillade",
                "J.",
                ""
            ],
            [
                "Z\u00e9rah",
                "G.",
                ""
            ],
            [
                "Bouchaud",
                "J. -Ph.",
                ""
            ]
        ]
    },
    {
        "id": "1501.07480",
        "submitter": "Oliver Janke",
        "authors": "Oliver Janke and Qinghua Li",
        "title": "Portfolio Optimization under Shortfall Risk Constraint",
        "comments": "25 pages, Optimization, 2016",
        "journal-ref": "Optimization, 65(9), 1733-1755 (2016)",
        "doi": "10.1080/02331934.2016.1173693",
        "report-no": null,
        "categories": "q-fin.MF q-fin.PM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper solves a utility maximization problem under utility-based\nshortfall risk constraint, by proposing an approach using Lagrange multiplier\nand convex duality. Under mild conditions on the asymptotic elasticity of the\nutility function and the loss function, we find an optimal wealth process for\nthe constrained problem and characterize the bi-dual relation between the\nrespective value functions of the constrained problem and its dual. This\napproach applies to both complete and incomplete markets. Moreover, the\nextension to more complicated cases is illustrated by solving the problem with\na consumption process added. Finally, we give an example of utility and loss\nfunctions in the Black-Scholes market where the solutions have explicit forms.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 29 Jan 2015 15:28:55 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 12 Feb 2015 08:15:44 GMT"
            },
            {
                "version": "v3",
                "created": "Tue, 2 Jun 2015 15:52:05 GMT"
            },
            {
                "version": "v4",
                "created": "Wed, 20 Apr 2016 07:00:02 GMT"
            }
        ],
        "update_date": "2016-06-28",
        "authors_parsed": [
            [
                "Janke",
                "Oliver",
                ""
            ],
            [
                "Li",
                "Qinghua",
                ""
            ]
        ]
    },
    {
        "id": "2005.03554",
        "submitter": "Yerkin Kitapbayev",
        "authors": "Yerkin Kitapbayev, Scott Robertson",
        "title": "Mortgage Contracts and Underwater Default",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.PR q-fin.RM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze recently proposed mortgage contracts that aim to eliminate\nselective borrower default when the loan balance exceeds the house price (the\n``underwater'' effect). We show contracts that automatically reduce the\noutstanding balance in the event of house price decline remove the default\nincentive, but may induce prepayment in low price states. However, low state\nprepayments vanish if the benefit from home ownership is sufficiently high. We\nalso show that capital gain sharing features, such as prepayment penalties in\nhigh house price states, are ineffective as they virtually eliminate\nprepayment. For observed foreclosure costs, we find that contracts with\nautomatic balance adjustments become preferable to the traditional fixed-rate\ncontracts at mortgage rate spreads between 20-50 basis points. We obtain these\nresults for perpetual versions of the contracts using American options pricing\nmethodology, in a continuous-time model with diffusive home prices. The\ncontracts' values and optimal decision rules are associated with free boundary\nproblems, which admit semi-explicit solutions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 7 May 2020 15:40:57 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 14 Jan 2021 20:43:01 GMT"
            },
            {
                "version": "v3",
                "created": "Sun, 30 May 2021 16:56:14 GMT"
            },
            {
                "version": "v4",
                "created": "Tue, 31 May 2022 17:54:02 GMT"
            }
        ],
        "update_date": "2022-06-01",
        "authors_parsed": [
            [
                "Kitapbayev",
                "Yerkin",
                ""
            ],
            [
                "Robertson",
                "Scott",
                ""
            ]
        ]
    },
    {
        "id": "2209.00534",
        "submitter": "Germ\\'an Reyes",
        "authors": "Marcel Preuss and Germ\\'an Reyes and Jason Somerville and Joy Wu",
        "title": "Inequality of Opportunity and Income Redistribution",
        "comments": "JEL codes: C91, D63",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.GN q-fin.EC",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We examine how redistribution decisions respond to the source of luck when\nthere is uncertainty about its role in determining opportunities and outcomes.\nWe elicit redistribution decisions from a representative U.S. sample who\nobserve worker outcomes and whether luck could determine earnings directly\n(``lucky outcomes'') or indirectly by providing one of the workers with a\nrelative advantage (``lucky opportunities''). We find that participants\nredistribute less and are less responsive to changes in the importance of luck\nin environments with lucky opportunities. We show that individuals rely on a\nsimple heuristic when assessing the impact of unequal opportunities, which\nleads them to underappreciate the extent to which small differences in\nopportunities can have a large impact on outcomes. These findings have\nimplications for models of redistribution attitudes and help explain the gap\nbetween lab evidence on support for redistribution and inequality trends.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 1 Sep 2022 15:28:53 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 2 Dec 2022 20:36:07 GMT"
            },
            {
                "version": "v3",
                "created": "Sat, 17 Jun 2023 14:26:44 GMT"
            },
            {
                "version": "v4",
                "created": "Tue, 12 Dec 2023 14:30:39 GMT"
            },
            {
                "version": "v5",
                "created": "Wed, 13 Mar 2024 20:21:25 GMT"
            }
        ],
        "update_date": "2024-03-15",
        "authors_parsed": [
            [
                "Preuss",
                "Marcel",
                ""
            ],
            [
                "Reyes",
                "Germ\u00e1n",
                ""
            ],
            [
                "Somerville",
                "Jason",
                ""
            ],
            [
                "Wu",
                "Joy",
                ""
            ]
        ]
    },
    {
        "id": "1912.10488",
        "submitter": "Jason Blevins",
        "authors": "Adam Dearing and Jason R. Blevins",
        "title": "Efficient and Convergent Sequential Pseudo-Likelihood Estimation of\n  Dynamic Discrete Games",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.EM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new sequential Efficient Pseudo-Likelihood (k-EPL) estimator for\ndynamic discrete choice games of incomplete information. k-EPL considers the\njoint behavior of multiple players simultaneously, as opposed to individual\nresponses to other agents' equilibrium play. This, in addition to reframing the\nproblem from conditional choice probability (CCP) space to value function\nspace, yields a computationally tractable, stable, and efficient estimator. We\nshow that each iteration in the k-EPL sequence is consistent and asymptotically\nefficient, so the first-order asymptotic properties do not vary across\niterations. Furthermore, we show the sequence achieves higher-order equivalence\nto the finite-sample maximum likelihood estimator with iteration and that the\nsequence of estimators converges almost surely to the maximum likelihood\nestimator at a nearly-superlinear rate when the data are generated by any\nregular Markov perfect equilibrium, including equilibria that lead to\ninconsistency of other sequential estimators. When utility is linear in\nparameters, k-EPL iterations are computationally simple, only requiring that\nthe researcher solve linear systems of equations to generate pseudo-regressors\nwhich are used in a static logit/probit regression. Monte Carlo simulations\ndemonstrate the theoretical results and show k-EPL's good performance in finite\nsamples in both small- and large-scale games, even when the game admits\nspurious equilibria in addition to one that generated the data. We apply the\nestimator to study the role of competition in the U.S. wholesale club industry.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 22 Dec 2019 17:34:23 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 9 Feb 2021 05:04:03 GMT"
            },
            {
                "version": "v3",
                "created": "Sun, 21 May 2023 03:22:46 GMT"
            },
            {
                "version": "v4",
                "created": "Mon, 27 Nov 2023 22:07:21 GMT"
            },
            {
                "version": "v5",
                "created": "Wed, 29 Nov 2023 22:31:34 GMT"
            },
            {
                "version": "v6",
                "created": "Wed, 24 Apr 2024 20:12:42 GMT"
            }
        ],
        "update_date": "2024-04-26",
        "authors_parsed": [
            [
                "Dearing",
                "Adam",
                ""
            ],
            [
                "Blevins",
                "Jason R.",
                ""
            ]
        ]
    },
    {
        "id": "1912.03270",
        "submitter": "Sai Srikar Nimmagadda",
        "authors": "Sai Srikar Nimmagadda, Pawan Sasanka Ammanamanchi",
        "title": "BitMEX Funding Correlation with Bitcoin Exchange Rate",
        "comments": "9 pages,5 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.ST",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the relationship between Inverse Perpetual Swap\ncontracts, a Bitcoin derivative akin to futures and the margin funding interest\nrates levied on BitMEX. This paper proves the Heteroskedastic nature of funding\nrates and goes onto establish a causal relationship between the funding rates\nand the Bitcoin inverse Perpetual swap contracts based on Granger causality.\nThe paper further dwells into developing a predictive model for funding rates\nusing best-fitted GARCH models. Implications of the results are presented, and\nfunding rates as a predictive tool for gauging the market trend is discussed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 26 Nov 2019 09:35:31 GMT"
            }
        ],
        "update_date": "2019-12-09",
        "authors_parsed": [
            [
                "Nimmagadda",
                "Sai Srikar",
                ""
            ],
            [
                "Ammanamanchi",
                "Pawan Sasanka",
                ""
            ]
        ]
    },
    {
        "id": "2004.06144",
        "submitter": "Youssef Nassef",
        "authors": "Youssef Nassef",
        "title": "The PCL Framework: A strategic approach to comprehensive risk management\n  in response to climate change impacts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.RM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The PCL framework provides a comprehensive climate risk management approach\ngrounded in the assessment of societal values of financial and non-financial\nloss tolerability. The framework optimizes response action across three main\nclusters, namely preemptive adaptation (P) or risk reduction, contingent\narrangements (C), and loss acceptance (L); without a predetermined hierarchy\nacross them. The PCL Framework aims at including the three clusters of outlay\nwithin a single continuum, and with the main policy outcome being a balanced\nportfolio of actions across the three clusters by way of an optimization\nmodule, such that the aggregate outlay is optimized in the long-term. It is\nproposed that the approach be applied separately for each hazard to which the\ntarget community is exposed. While it is currently applied to climate-related\nrisk management, the methodology can be repurposed for use in other contexts\nwhere societal buy-in is central.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 13 Apr 2020 18:24:34 GMT"
            }
        ],
        "update_date": "2020-04-15",
        "authors_parsed": [
            [
                "Nassef",
                "Youssef",
                ""
            ]
        ]
    },
    {
        "id": "2109.06759",
        "submitter": "Louis Charlot",
        "authors": "Louis Charlot",
        "title": "Bayesian hierarchical analysis of a multifaceted program against extreme\n  poverty",
        "comments": "Master Thesis",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.EM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The evaluation of a multifaceted program against extreme poverty in different\ndeveloping countries gave encouraging results, but with important heterogeneity\nbetween countries. This master thesis proposes to study this heterogeneity with\na Bayesian hierarchical analysis. The analysis we carry out with two different\nhierarchical models leads to a very low amount of pooling of information\nbetween countries, indicating that this observed heterogeneity should be\ninterpreted mostly as true heterogeneity, and not as sampling error. We analyze\nthe first order behavior of our hierarchical models, in order to understand\nwhat leads to this very low amount of pooling. We try to give to this work a\ndidactic approach, with an introduction of Bayesian analysis and an explanation\nof the different modeling and computational choices of our analysis.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 14 Sep 2021 15:25:36 GMT"
            }
        ],
        "update_date": "2021-09-15",
        "authors_parsed": [
            [
                "Charlot",
                "Louis",
                ""
            ]
        ]
    },
    {
        "id": "1904.07444",
        "submitter": "Hyo Seok Jang",
        "authors": "Hyo Seok Jang, Sangjik Lee",
        "title": "Equilibria in a large production economy with an infinite dimensional\n  commodity space and price dependent preferences",
        "comments": "JEL Classification Numbers: C62, D51. Keywords: Separable Banach\n  space, Saturated measure space, Price dependent preferences, Lyapunov's\n  convexity theorem, Fatou's lemma",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.TH math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove the existence of a competitive equilibrium in a production economy\nwith infinitely many commodities and a measure space of agents whose\npreferences are price dependent. We employ a saturated measure space for the\nset of agents and apply recent results for an infinite dimensional separable\nBanach space such as Lyapunov's convexity theorem and an exact Fatou's lemma to\nobtain the result.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 16 Apr 2019 03:57:58 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 5 Feb 2020 04:53:54 GMT"
            }
        ],
        "update_date": "2020-02-06",
        "authors_parsed": [
            [
                "Jang",
                "Hyo Seok",
                ""
            ],
            [
                "Lee",
                "Sangjik",
                ""
            ]
        ]
    },
    {
        "id": "1201.6130",
        "submitter": "Peter Kratz",
        "authors": "Peter Kratz, Torsten Sch\\\"oneborn",
        "title": "Portfolio liquidation in dark pools in continuous time",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.TR math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an illiquid financial market where a risk averse investor has to\nliquidate a portfolio within a finite time horizon [0,T] and can trade\ncontinuously at a traditional exchange (the \"primary venue\") and in a dark\npool. At the primary venue, trading yields a linear price impact. In the dark\npool, no price impact costs arise but order execution is uncertain, modeled by\na multi-dimensional Poisson process. We characterize the costs of trading by a\nlinear-quadratic functional which incorporates both the price impact costs of\ntrading at the primary exchange and the market risk of the position. The\nliquidation constraint implies a singularity of the value function of the\nresulting minimization problem at the terminal time T. Via the HJB equation and\na quadratic ansatz, we obtain a candidate for the value function which is the\nlimit of a sequence of solutions of initial value problems for a matrix\ndifferential equation. We show that this limit exists by using an appropriate\nmatrix inequality and a comparison result for Riccati matrix equations.\nAdditionally, we obtain upper and lower bounds of the solutions of the initial\nvalue problems, which allow us to prove a verification theorem. If a single\nasset position is to be liquidated, the investor slowly trades out of her\nposition at the primary venue, with the remainder being placed in the dark pool\nat any point in time. For multi-asset liquidations this is generally not the\ncase; it can, e.g., be optimal to oversize orders in the dark pool in order to\nturn a poorly balanced portfolio into a portfolio bearing less risk.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 30 Jan 2012 08:33:32 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 6 Aug 2012 09:56:44 GMT"
            }
        ],
        "update_date": "2012-08-07",
        "authors_parsed": [
            [
                "Kratz",
                "Peter",
                ""
            ],
            [
                "Sch\u00f6neborn",
                "Torsten",
                ""
            ]
        ]
    },
    {
        "id": "0805.3397",
        "submitter": "Matus Medo",
        "authors": "Matus Medo, Chi Ho Yeung, Yi-Cheng Zhang",
        "title": "How to quantify the influence of correlations on investment\n  diversification",
        "comments": "14 pages, 4 figures",
        "journal-ref": "International Review of Financial Analysis 18, 34-39 (2009)",
        "doi": "10.1016/j.irfa.2009.01.001",
        "report-no": null,
        "categories": "q-fin.PM physics.soc-ph",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When assets are correlated, benefits of investment diversification are\nreduced. To measure the influence of correlations on investment performance, a\nnew quantity - the effective portfolio size - is proposed and investigated in\nboth artificial and real situations. We show that in most cases, the effective\nportfolio size is much smaller than the actual number of assets in the\nportfolio and that it lowers even further during financial crises.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 22 May 2008 07:38:06 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 16 Feb 2009 09:11:03 GMT"
            }
        ],
        "update_date": "2009-04-16",
        "authors_parsed": [
            [
                "Medo",
                "Matus",
                ""
            ],
            [
                "Yeung",
                "Chi Ho",
                ""
            ],
            [
                "Zhang",
                "Yi-Cheng",
                ""
            ]
        ]
    },
    {
        "id": "2304.04453",
        "submitter": "Claudio Fontana",
        "authors": "Claudio Fontana, Simone Pavarana, Wolfgang J. Runggaldier",
        "title": "A stochastic control perspective on term structure models with roll-over\n  risk",
        "comments": "25 pages (revised version)",
        "journal-ref": "Finance and Stochastics (2023), 27: 903-932",
        "doi": "10.1007/s00780-023-00515-z",
        "report-no": null,
        "categories": "q-fin.PR math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider a generic interest rate market in the presence of\nroll-over risk, which generates spreads in spot/forward term rates. We do not\nrequire classical absence of arbitrage and rely instead on a minimal market\nviability assumption, which enables us to work in the context of the benchmark\napproach. In a Markovian setting, we extend the control theoretic approach of\nGombani & Runggaldier (2013) and derive representations of spot/forward spreads\nas value functions of suitable stochastic optimal control problems, formulated\nunder the real-world probability and with power-type objective functionals. We\ndetermine endogenously the funding-liquidity spread by relating it to the\nrisk-sensitive optimization problem of a representative investor.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 10 Apr 2023 08:39:17 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 5 Oct 2023 06:18:41 GMT"
            }
        ],
        "update_date": "2023-10-06",
        "authors_parsed": [
            [
                "Fontana",
                "Claudio",
                ""
            ],
            [
                "Pavarana",
                "Simone",
                ""
            ],
            [
                "Runggaldier",
                "Wolfgang J.",
                ""
            ]
        ]
    },
    {
        "id": "2111.03995",
        "submitter": "Mao Guan",
        "authors": "Mao Guan, Xiao-Yang Liu",
        "title": "Explainable Deep Reinforcement Learning for Portfolio Management: An\n  Empirical Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.PM cs.AI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Deep reinforcement learning (DRL) has been widely studied in the portfolio\nmanagement task. However, it is challenging to understand a DRL-based trading\nstrategy because of the black-box nature of deep neural networks. In this\npaper, we propose an empirical approach to explain the strategies of DRL agents\nfor the portfolio management task. First, we use a linear model in hindsight as\nthe reference model, which finds the best portfolio weights by assuming knowing\nactual stock returns in foresight. In particular, we use the coefficients of a\nlinear model in hindsight as the reference feature weights. Secondly, for DRL\nagents, we use integrated gradients to define the feature weights, which are\nthe coefficients between reward and features under a linear regression model.\nThirdly, we study the prediction power in two cases, single-step prediction and\nmulti-step prediction. In particular, we quantify the prediction power by\ncalculating the linear correlations between the feature weights of a DRL agent\nand the reference feature weights, and similarly for machine learning methods.\nFinally, we evaluate a portfolio management task on Dow Jones 30 constituent\nstocks during 01/01/2009 to 09/01/2021. Our approach empirically reveals that a\nDRL agent exhibits a stronger multi-step prediction power than machine learning\nmethods.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 7 Nov 2021 04:23:48 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 18 Dec 2021 22:51:40 GMT"
            }
        ],
        "update_date": "2021-12-21",
        "authors_parsed": [
            [
                "Guan",
                "Mao",
                ""
            ],
            [
                "Liu",
                "Xiao-Yang",
                ""
            ]
        ]
    },
    {
        "id": "2307.14218",
        "submitter": "Antoine Jacquier Dr.",
        "authors": "Antoine Jacquier and Mugad Oumgari",
        "title": "Interest rate convexity in a Gaussian framework",
        "comments": "17 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.PR math.PR",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The contributions of this paper are twofold: we define and investigate the\nproperties of a short rate model driven by a general Gaussian Volterra process\nand, after defining precisely a notion of convexity adjustment, derive explicit\nformulae for it.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 26 Jul 2023 14:35:28 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 13 Mar 2024 12:14:25 GMT"
            }
        ],
        "update_date": "2024-03-14",
        "authors_parsed": [
            [
                "Jacquier",
                "Antoine",
                ""
            ],
            [
                "Oumgari",
                "Mugad",
                ""
            ]
        ]
    },
    {
        "id": "1004.1855",
        "submitter": "Luca Capriotti",
        "authors": "Luca Capriotti and Mike Giles",
        "title": "Fast Correlation Greeks by Adjoint Algorithmic Differentiation",
        "comments": "5 pages, 2 figures",
        "journal-ref": "Risk Magazine, April 2010",
        "doi": null,
        "report-no": null,
        "categories": "q-fin.CP",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show how Adjoint Algorithmic Differentiation (AAD) allows an extremely\nefficient calculation of correlation Risk of option prices computed with Monte\nCarlo simulations. A key point in the construction is the use of binning to\nsimultaneously achieve computational efficiency and accurate confidence\nintervals. We illustrate the method for a copula-based Monte Carlo computation\nof claims written on a basket of underlying assets, and we test it numerically\nfor Portfolio Default Options. For any number of underlying assets or names in\na portfolio, the sensitivities of the option price with respect to all the\npairwise correlations is obtained at a computational cost which is at most 4\ntimes the cost of calculating the option value itself. For typical\napplications, this results in computational savings of several order of\nmagnitudes with respect to standard methods.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 11 Apr 2010 23:42:01 GMT"
            }
        ],
        "update_date": "2010-04-13",
        "authors_parsed": [
            [
                "Capriotti",
                "Luca",
                ""
            ],
            [
                "Giles",
                "Mike",
                ""
            ]
        ]
    },
    {
        "id": "0807.1818",
        "submitter": "Fei Ren",
        "authors": "Fei Ren, Liang Guo, and Wei-Xing Zhou",
        "title": "Statistical properties of volatility return intervals of Chinese stocks",
        "comments": "8 pages, 8 figures",
        "journal-ref": "Physica A 388 (6), 881-890 (2009)",
        "doi": "10.1016/j.physa.2008.12.005",
        "report-no": null,
        "categories": "q-fin.ST physics.data-an physics.soc-ph",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The statistical properties of the return intervals $\\tau_q$ between\nsuccessive 1-min volatilities of 30 liquid Chinese stocks exceeding a certain\nthreshold $q$ are carefully studied. The Kolmogorov-Smirnov (KS) test shows\nthat 12 stocks exhibit scaling behaviors in the distributions of $\\tau_q$ for\ndifferent thresholds $q$. Furthermore, the KS test and weighted KS test shows\nthat the scaled return interval distributions of 6 stocks (out of the 12\nstocks) can be nicely fitted by a stretched exponential function\n$f(\\tau/\\bar{\\tau})\\sim e^{- \\alpha (\\tau/\\bar{\\tau})^{\\gamma}}$ with\n$\\gamma\\approx0.31$ under the significance level of 5%, where $\\bar{\\tau}$ is\nthe mean return interval. The investigation of the conditional probability\ndistribution $P_q(\\tau | \\tau_0)$ and the mean conditional return interval\n$<\\tau| \\tau_0>$ demonstrates the existence of short-term correlation between\nsuccessive return interval intervals. We further study the mean return interval\n$<\\tau| \\tau_0>$ after a cluster of $n$ intervals and the fluctuation $F(l)$\nusing detrended fluctuation analysis and find that long-term memory also exists\nin the volatility return intervals.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 11 Jul 2008 10:21:39 GMT"
            }
        ],
        "update_date": "2009-01-09",
        "authors_parsed": [
            [
                "Ren",
                "Fei",
                ""
            ],
            [
                "Guo",
                "Liang",
                ""
            ],
            [
                "Zhou",
                "Wei-Xing",
                ""
            ]
        ]
    },
    {
        "id": "1906.00553",
        "submitter": "Martin Obschonka",
        "authors": "Martin Obschonka, David B. Audretsch",
        "title": "Artificial Intelligence and Big Data in Entrepreneurship: A New Era Has\n  Begun",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s11187-019-00202-4",
        "report-no": null,
        "categories": "econ.GN q-fin.EC",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  While the disruptive potential of artificial intelligence (AI) and Big Data\nhas been receiving growing attention and concern in a variety of research and\napplication fields over the last few years, it has not received much scrutiny\nin contemporary entrepreneurship research so far. Here we present some\nreflections and a collection of papers on the role of AI and Big Data for this\nemerging area in the study and application of entrepreneurship research. While\nbeing mindful of the potentially overwhelming nature of the rapid progress in\nmachine intelligence and other Big Data technologies for contemporary\nstructures in entrepreneurship research, we put an emphasis on the reciprocity\nof the co-evolving fields of entrepreneurship research and practice. How can AI\nand Big Data contribute to a productive transformation of the research field\nand the real-world phenomena (e.g., 'smart entrepreneurship')? We also discuss,\nhowever, ethical issues as well as challenges around a potential contradiction\nbetween entrepreneurial uncertainty and rule-driven AI rationality. The\neditorial gives researchers and practitioners orientation and showcases avenues\nand examples for concrete research in this field. At the same time, however, it\nis not unlikely that we will encounter unforeseeable and currently inexplicable\ndevelopments in the field soon. We call on entrepreneurship scholars,\neducators, and practitioners to proactively prepare for future scenarios.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 3 Jun 2019 03:52:47 GMT"
            }
        ],
        "update_date": "2019-06-04",
        "authors_parsed": [
            [
                "Obschonka",
                "Martin",
                ""
            ],
            [
                "Audretsch",
                "David B.",
                ""
            ]
        ]
    },
    {
        "id": "1501.06980",
        "submitter": "Masaaki Fukasawa",
        "authors": "Masaaki Fukasawa",
        "title": "Short-time at-the-money skew and rough fractional volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.MF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Black-Scholes implied volatility skew at the money of SPX options is\nknown to obey a power law with respect to the time-to-maturity. We construct a\nmodel of the underlying asset price process which is dynamically consistent to\nthe power law. The volatility process of the model is driven by a fractional\nBrownian motion with Hurst parameter less than half. The fractional Brownian\nmotion is correlated with a Brownian motion which drives the asset price\nprocess. We derive an asymptotic expansion of the implied volatility as the\ntime-to-maturity tends to zero. For this purpose we introduce a new approach to\nvalidate such an expansion, which enables us to treat more general models than\nin the literature. The local-stochastic volatility model is treated as well\nunder an essentially minimal regularity condition in order to show such a\nstandard model cannot be dynamically consistent to the power law.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 28 Jan 2015 03:11:10 GMT"
            }
        ],
        "update_date": "2015-01-29",
        "authors_parsed": [
            [
                "Fukasawa",
                "Masaaki",
                ""
            ]
        ]
    },
    {
        "id": "2301.05886",
        "submitter": "Jonathan Spence",
        "authors": "Michael B. Giles, Abdul-Lateef Haji-Ali, Jonathan Spence",
        "title": "Efficient Risk Estimation for the Credit Valuation Adjustment",
        "comments": "35 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.CP",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The valuation of over-the-counter derivatives is subject to a series of\nvaluation adjustments known as xVA, which pose additional risks for financial\ninstitutions. Associated risk measures, such as the value-at-risk of an\nunderlying valuation adjustment, play an important role in managing these\nrisks. Monte Carlo methods are often regarded as inefficient for computing such\nmeasures. As an example, we consider the value-at-risk of the Credit Valuation\nAdjustment (CVA-VaR), which can be expressed using a triple nested expectation.\nTraditional Monte Carlo methods are often inefficient at handling several\nnested expectations. Utilising recent developments in multilevel nested\nsimulation for probabilities, we construct a hierarchical estimator of the\nCVA-VaR which reduces the computational complexity by 3 orders of magnitude\ncompared to standard Monte Carlo.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 14 Jan 2023 10:49:10 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 23 May 2024 14:11:19 GMT"
            }
        ],
        "update_date": "2024-05-24",
        "authors_parsed": [
            [
                "Giles",
                "Michael B.",
                ""
            ],
            [
                "Haji-Ali",
                "Abdul-Lateef",
                ""
            ],
            [
                "Spence",
                "Jonathan",
                ""
            ]
        ]
    },
    {
        "id": "2008.07103",
        "submitter": "Shengchao Zhuang",
        "authors": "Yichun Chi, Xun Yu Zhou and Sheng Chao Zhuang",
        "title": "Variance Contracts",
        "comments": "42 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.RM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the design of an optimal insurance contract in which the insured\nmaximizes her expected utility and the insurer limits the variance of his risk\nexposure while maintaining the principle of indemnity and charging the premium\naccording to the expected value principle. We derive the optimal policy\nsemi-analytically, which is coinsurance above a deductible when the variance\nbound is binding. This policy automatically satisfies the incentive-compatible\ncondition, which is crucial to rule out ex post moral hazard. We also find that\nthe deductible is absent if and only if the contract pricing is actuarially\nfair. Focusing on the actuarially fair case, we carry out comparative statics\non the effects of the insured's initial wealth and the variance bound on\ninsurance demand. Our results indicate that the expected coverage is always\nlarger for a wealthier insured, implying that the underlying insurance is a\nnormal good, which supports certain recent empirical findings. Moreover, as the\nvariance constraint tightens, the insured who is prudent cedes less losses,\nwhile the insurer is exposed to less tail risk.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 17 Aug 2020 05:58:59 GMT"
            }
        ],
        "update_date": "2020-08-18",
        "authors_parsed": [
            [
                "Chi",
                "Yichun",
                ""
            ],
            [
                "Zhou",
                "Xun Yu",
                ""
            ],
            [
                "Zhuang",
                "Sheng Chao",
                ""
            ]
        ]
    },
    {
        "id": "2010.12736",
        "submitter": "Khanh Nguyen Q",
        "authors": "Khanh Q. Nguyen",
        "title": "Conditional beta and uncertainty factor in the cryptocurrency pricing\n  model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.GN q-fin.EC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This research is to assess cryptocurrencies with the conditional beta,\ncompared with prior studies based on unconditional beta or fixed beta. It is a\nnew approach to building a pricing model for cryptocurrencies. Therefore, we\nexpect that the use of conditional beta will increase the explanatory ability\nof factors in previous pricing models. Besides, this research is also a pioneer\nin placing the uncertainty factor in the cryptocurrency pricing model. Earlier\nstudies on cryptocurrency pricing have ignored this factor. However, it is a\nsignificant factor in the valuation of cryptocurrencies because uncertainty\nleads to investor sentiment and affects prices.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 24 Oct 2020 01:31:43 GMT"
            }
        ],
        "update_date": "2020-10-27",
        "authors_parsed": [
            [
                "Nguyen",
                "Khanh Q.",
                ""
            ]
        ]
    },
    {
        "id": "2202.08590",
        "submitter": "Siddhartha Paul Tiwari",
        "authors": "Siddhartha Paul Tiwari",
        "title": "Emerging trends in soybean industry",
        "comments": null,
        "journal-ref": "Soybean Research 15.1 (2017): 1-17",
        "doi": null,
        "report-no": null,
        "categories": "econ.GN q-fin.EC",
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Soybean is the most globalized, traded and processed crop commodity. USA,\nArgentina and Brazil continue to be the top three producers and exporters of\nsoybean and soymeal. Indian soyindustry has also made a mark in the national\nand global arena. While soymeal, soyoil, lecithin and other soy-derivatives\nstand to be driven up by commerce, the soyfoods for human health and nutrition\nneed to be further promoted. The changing habitat of commerce in soyderivatives\nnecessitates a shift in strategy, technological tools and policy environment to\nmake Indian soybean industry continue to thrive in the new industrial era.\nTerms of trade for soyfarming and soy-industry could be further improved.\nPresent trends, volatilities, slowdowns, challenges faced and associated\ndesiderata are accordingly spelt out in the present article.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 17 Feb 2022 11:15:19 GMT"
            }
        ],
        "update_date": "2022-02-21",
        "authors_parsed": [
            [
                "Tiwari",
                "Siddhartha Paul",
                ""
            ]
        ]
    },
    {
        "id": "1710.11432",
        "submitter": "Qizhu Liang",
        "authors": "Qizhu Liang and Jie Xiong",
        "title": "Stochastic maximum principle under probability distortion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.MF math.PR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Within the framework of the cumulative prospective theory of Kahneman and\nTversky, this paper considers a continuous-time behavioral portfolio selection\nproblem whose model includes both running and terminal terms in the objective\nfunctional. Despite the existence of S-shaped utility functions and probability\ndistortions, a necessary condition for optimality is derived. The results are\napplied to various examples.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 31 Oct 2017 12:35:55 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 23 Aug 2018 02:27:07 GMT"
            }
        ],
        "update_date": "2018-08-24",
        "authors_parsed": [
            [
                "Liang",
                "Qizhu",
                ""
            ],
            [
                "Xiong",
                "Jie",
                ""
            ]
        ]
    },
    {
        "id": "0901.1315",
        "submitter": "Enrique ter Horst A",
        "authors": "Abel Rodriguez and Henryk Gzyl and German Molina and Enrique ter Horst",
        "title": "Stochastic Volatility Models Including Open, Close, High and Low Prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.ST cs.CE cs.NA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mounting empirical evidence suggests that the observed extreme prices within\na trading period can provide valuable information about the volatility of the\nprocess within that period. In this paper we define a class of stochastic\nvolatility models that uses opening and closing prices along with the minimum\nand maximum prices within a trading period to infer the dynamics underlying the\nvolatility process of asset prices and compares it with similar models that\nhave been previously presented in the literature. The paper also discusses\nsequential Monte Carlo algorithms to fit this class of models and illustrates\nits features using both a simulation study and data form the SP500 index.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 9 Jan 2009 21:08:29 GMT"
            }
        ],
        "update_date": "2009-01-12",
        "authors_parsed": [
            [
                "Rodriguez",
                "Abel",
                ""
            ],
            [
                "Gzyl",
                "Henryk",
                ""
            ],
            [
                "Molina",
                "German",
                ""
            ],
            [
                "ter Horst",
                "Enrique",
                ""
            ]
        ]
    },
    {
        "id": "1406.2950",
        "submitter": "Hirbod Assa",
        "authors": "Hirbod Assa",
        "title": "On Optimal Reinsurance Policy with Distortion Risk Measures and Premiums",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.RM math.PR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider the problem of optimal reinsurance design, when\nthe risk is measured by a distortion risk measure and the premium is given by a\ndistortion risk premium. First, we show how the optimal reinsurance design for\nthe ceding company, the reinsurance company and the social planner can be\nformulated in the same way. Second, by introducing the marginal indemnification\nfunctions, we characterize the optimal reinsurance contracts. We show that, for\nan optimal policy, the associated marginal indemnification function only takes\nthe values zero and one. We will see how the roles of the market preferences\nand premiums and that of the total risk are separated.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Jun 2014 16:09:15 GMT"
            }
        ],
        "update_date": "2014-06-12",
        "authors_parsed": [
            [
                "Assa",
                "Hirbod",
                ""
            ]
        ]
    },
    {
        "id": "1906.10311",
        "submitter": "Takeshi Nishimura",
        "authors": "Takeshi Nishimura",
        "title": "Informed Principal Problems in Bilateral Trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.TH",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study bilateral trade with interdependent values as an informed-principal\nproblem. The mechanism-selection game has multiple equilibria that differ with\nrespect to principal's payoff and trading surplus. We characterize the\nequilibrium that is worst for every type of principal, and characterize the\nconditions under which there are no equilibria with different payoffs for the\nprincipal. We also show that this is the unique equilibrium that survives the\nintuitive criterion.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 25 Jun 2019 03:33:54 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 17 Jul 2019 10:04:23 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 5 Oct 2020 04:20:40 GMT"
            },
            {
                "version": "v4",
                "created": "Tue, 1 Jun 2021 13:38:02 GMT"
            },
            {
                "version": "v5",
                "created": "Wed, 29 Dec 2021 12:47:58 GMT"
            },
            {
                "version": "v6",
                "created": "Mon, 21 Feb 2022 15:09:23 GMT"
            }
        ],
        "update_date": "2022-02-22",
        "authors_parsed": [
            [
                "Nishimura",
                "Takeshi",
                ""
            ]
        ]
    },
    {
        "id": "2105.13341",
        "submitter": "Andrew Ellis",
        "authors": "Andrew Ellis",
        "title": "Correlation Concern",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.TH",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In many choice problems, the interaction between several distinct variables\ndetermines the payoff of each alternative. I propose and axiomatize a model of\na decision maker who recognizes that she may not accurately perceive the\ncorrelation between these variables, and who takes this into account when\nmaking her decision. She chooses as if she calculates each alternative's\nexpected outcome under multiple possible correlation structures, and then\nevaluates it according to the worst expected outcome.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 27 May 2021 17:48:30 GMT"
            }
        ],
        "update_date": "2021-05-28",
        "authors_parsed": [
            [
                "Ellis",
                "Andrew",
                ""
            ]
        ]
    },
    {
        "id": "1204.5103",
        "submitter": "Anirban Chakraborti",
        "authors": "Gayatri Tilak, Tamas Szell, Remy Chicheportiche and Anirban\n  Chakraborti",
        "title": "Study of statistical correlations in intraday and daily financial return\n  time series",
        "comments": "22 pages, 11 figures, Springer-Verlag format. To appear in the\n  conference proceedings of Econophys-Kolkata VI: \"Econophysics of systemic\n  risk and network dynamics\", Eds. F. Abergel, B.K. Chakrabarti, A. Chakraborti\n  and A. Ghosh, to be published by Springer-Verlag (Italia), Milan (2012)",
        "journal-ref": null,
        "doi": "10.1007/978-88-470-2553-0_6",
        "report-no": null,
        "categories": "q-fin.ST",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this article is to briefly review and make new studies of\ncorrelations and co-movements of stocks, so as to understand the\n\"seasonalities\" and market evolution. Using the intraday data of the CAC40, we\nbegin by reasserting the findings of Allez and Bouchaud [New J. Phys. 13,\n025010 (2011)]: the average correlation between stocks increases throughout the\nday. We then use multidimensional scaling (MDS) in generating maps and\nvisualizing the dynamic evolution of the stock market during the day. We do not\nfind any marked difference in the structure of the market during a day. Another\naim is to use daily data for MDS studies, and visualize or detect specific\nsectors in a market and periods of crisis. We suggest that this type of\nvisualization may be used in identifying potential pairs of stocks for \"pairs\ntrade\".\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 23 Apr 2012 16:42:09 GMT"
            }
        ],
        "update_date": "2015-06-04",
        "authors_parsed": [
            [
                "Tilak",
                "Gayatri",
                ""
            ],
            [
                "Szell",
                "Tamas",
                ""
            ],
            [
                "Chicheportiche",
                "Remy",
                ""
            ],
            [
                "Chakraborti",
                "Anirban",
                ""
            ]
        ]
    },
    {
        "id": "2105.04171",
        "submitter": "Geoffrey Ducournau",
        "authors": "Geoffrey Ducournau",
        "title": "Bayesian inference and superstatistics to describe long memory processes\n  of financial time series",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.ST",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the standardized features of financial data is that log-returns are\nuncorrelated, but absolute log-returns or their squares namely the fluctuating\nvolatility are correlated and is characterized by heavy tailed in the sense\nthat some moment of the absolute log-returns is infinite and typically\nnon-Gaussian [20]. And this last characteristic change accordantly to different\ntimescales. We propose to model this long-memory phenomenon by superstatistical\ndynamics and provide a Bayesian Inference methodology drawing on\nMetropolis-Hasting random walk sampling to determine which superstatistics\namong inverse-Gamma and log-Normal describe the best log-returns complexity on\ndifferent timescales, from high to low frequency. We show that on smaller\ntimescales (minutes) even though the Inverse-Gamma superstatistics works the\nbest, the log-Normal model remains very reliable and suitable to fit the\nabsolute log-returns probability density distribution with strong capacity of\ndescribing heavy tails and power law decays. On larger timescales (daily), we\nshow in terms of Bayes factor that the inverse-Gamma superstatistics is\npreferred to the log-Normal model. We also show evidence of a transition of\nstatistics from power law decay on small timescales to exponential decay on\nlarge scale with less heavy tails meaning that on larger time scales the\nfluctuating volatility tend to be memoryless, consequently superstatistics\nbecomes less relevant.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 10 May 2021 08:04:41 GMT"
            }
        ],
        "update_date": "2021-05-11",
        "authors_parsed": [
            [
                "Ducournau",
                "Geoffrey",
                ""
            ]
        ]
    },
    {
        "id": "2010.02378",
        "submitter": "Fabian Ruthardt",
        "authors": "Niklas Potrafke, Fabian Ruthardt, Kaspar W\\\"uthrich",
        "title": "Protectionism and economic growth: Causal evidence from the first era of\n  globalization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "econ.GN q-fin.EC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate how protectionist policies influence economic growth. Our\nempirical strategy exploits an extraordinary tax scandal that gave rise to an\nunexpected change of government in Sweden. A free-trade majority in parliament\nwas overturned by a protectionist majority in 1887. The protectionist\ngovernment increased tariffs. We employ the synthetic control method to select\ncontrol countries against which economic growth in Sweden can be compared. We\ndo not find evidence suggesting that protectionist policies influenced economic\ngrowth and examine channels why. The new tariff laws increased government\nrevenue. However, the results do not suggest that the protectionist government\nstimulated the economy by increasing government expenditure.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 5 Oct 2020 22:50:28 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 27 Apr 2021 09:48:44 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 3 Mar 2022 19:44:51 GMT"
            }
        ],
        "update_date": "2022-03-07",
        "authors_parsed": [
            [
                "Potrafke",
                "Niklas",
                ""
            ],
            [
                "Ruthardt",
                "Fabian",
                ""
            ],
            [
                "W\u00fcthrich",
                "Kaspar",
                ""
            ]
        ]
    }
]